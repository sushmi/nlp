{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Language Model\n",
    "\n",
    "In this assignment, we will focus on building a language model using a text dataset of your choice. The objective is to train a model that can generate coherent and contextually relevant text based on a given input. Additionally, you will develop a simple web application to demonstrate the capabilities of your language model interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Dataset Acquisition - Your first task is to find a suitable text dataset. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Choose your dataset and provide a brief description. Ensure to source this dataset from reputable public databases or repositories. It is imperative to give proper credit to the dataset source in your documentation.\n",
    "\n",
    "Note: The dataset can be based on any theme such as Harry Potter, Star Wars, jokes, Isaac Asimov’s works, Thai stories, etc. The key requirement is that the dataset should be text-rich and suitable for language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import datasets, math, re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mimimum required torch version for MPS support \"1.12+\"\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# universal device selection: use gpu if available, else cpu\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")      # NVIDIA GPU\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")       # Apple Silicon GPU\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "def force_cpu_device():\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = force_cpu_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data - Theme : Shakespeare "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores the ability of an LSTM language model to predict and generate text in the style of Shakespeare. For this purpose, I use the publicly available text of Shakespeare’s works from the Project Gutenberg website. More information about Project Gutenberg is provided below:\n",
    "\n",
    "<i>Excerpt from Gutenberg site:</i>\n",
    "\n",
    "<b>About Project Gutenberg</b>\n",
    "\n",
    "Project Gutenberg is an online library of more than 75,000 free eBooks.\n",
    "\n",
    "Michael Hart, founder of Project Gutenberg, invented eBooks in 1971 and his memory continues to inspire the creation of eBooks and related content today.\n",
    "\n",
    "Since then, thousands of volunteers have digitized and diligently proofread the world’s literature. The entire Project Gutenberg collection is yours to enjoy.\n",
    "\n",
    "All Project Gutenberg eBooks are completely free and always will be.\n",
    "\n",
    "\n",
    "Text used for training : [The Project Gutenberg eBook of The Complete Works of William Shakespeare\n",
    "](https://www.gutenberg.org/cache/epub/100/pg100.txt)\n",
    "\n",
    "<details>\n",
    "<summary>Contents </summary>\n",
    "\n",
    "    THE SONNETS\n",
    "    ALL’S WELL THAT ENDS WELL\n",
    "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
    "    AS YOU LIKE IT\n",
    "    THE COMEDY OF ERRORS\n",
    "    THE TRAGEDY OF CORIOLANUS\n",
    "    CYMBELINE\n",
    "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
    "    THE FIRST PART OF KING HENRY THE FOURTH\n",
    "    THE SECOND PART OF KING HENRY THE FOURTH\n",
    "    THE LIFE OF KING HENRY THE FIFTH\n",
    "    THE FIRST PART OF HENRY THE SIXTH\n",
    "    THE SECOND PART OF KING HENRY THE SIXTH\n",
    "    THE THIRD PART OF KING HENRY THE SIXTH\n",
    "    KING HENRY THE EIGHTH\n",
    "    THE LIFE AND DEATH OF KING JOHN\n",
    "    THE TRAGEDY OF JULIUS CAESAR\n",
    "    THE TRAGEDY OF KING LEAR\n",
    "    LOVE’S LABOUR’S LOST\n",
    "    THE TRAGEDY OF MACBETH\n",
    "    MEASURE FOR MEASURE\n",
    "    THE MERCHANT OF VENICE\n",
    "    THE MERRY WIVES OF WINDSOR\n",
    "    A MIDSUMMER NIGHT’S DREAM\n",
    "    MUCH ADO ABOUT NOTHING\n",
    "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
    "    PERICLES, PRINCE OF TYRE\n",
    "    KING RICHARD THE SECOND\n",
    "    KING RICHARD THE THIRD\n",
    "    THE TRAGEDY OF ROMEO AND JULIET\n",
    "    THE TAMING OF THE SHREW\n",
    "    THE TEMPEST\n",
    "    THE LIFE OF TIMON OF ATHENS\n",
    "    THE TRAGEDY OF TITUS ANDRONICUS\n",
    "    TROILUS AND CRESSIDA\n",
    "    TWELFTH NIGHT; OR, WHAT YOU WILL\n",
    "    THE TWO GENTLEMEN OF VERONA\n",
    "    THE TWO NOBLE KINSMEN\n",
    "    THE WINTER’S TALE\n",
    "    A LOVER’S COMPLAINT\n",
    "    THE PASSIONATE PILGRIM\n",
    "    THE PHOENIX AND THE TURTLE\n",
    "    THE RAPE OF LUCRECE\n",
    "    VENUS AND ADONIS\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Complete Works of William Shakespeare\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The Complete Works of William Shakespeare\n",
      "\n",
      "Author: William Shakespeare\n",
      "\n",
      "Release date: January 1, 1994 [eBook #100]\n",
      "                Most recently updated: August 24, 2025\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Complete Works of William Shakespeare\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Contents\n",
      "\n",
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "DATA_LOCAL_PATH = \"../data/gutenberg_pg100.txt\"\n",
    "\n",
    "# Download if file doesn't exist locally\n",
    "if not os.path.exists(DATA_LOCAL_PATH):\n",
    "    url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "    # Save to a local file\n",
    "    with open(DATA_LOCAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "else:\n",
    "    with open(DATA_LOCAL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "print(text[:1000])  # Print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of works: 44\n"
     ]
    }
   ],
   "source": [
    "shakespeare_content = [\n",
    "    \"THE SONNETS\",\n",
    "    \"ALL’S WELL THAT ENDS WELL\",\n",
    "    \"THE TRAGEDY OF ANTONY AND CLEOPATRA\",\n",
    "    \"AS YOU LIKE IT\",\n",
    "    \"THE COMEDY OF ERRORS\",\n",
    "    \"THE TRAGEDY OF CORIOLANUS\",\n",
    "    \"CYMBELINE\",\n",
    "    \"THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\",\n",
    "    \"THE FIRST PART OF KING HENRY THE FOURTH\",\n",
    "    \"THE SECOND PART OF KING HENRY THE FOURTH\",\n",
    "    \"THE LIFE OF KING HENRY THE FIFTH\",\n",
    "    \"THE FIRST PART OF HENRY THE SIXTH\",\n",
    "    \"THE SECOND PART OF KING HENRY THE SIXTH\",\n",
    "    \"THE THIRD PART OF KING HENRY THE SIXTH\",\n",
    "    \"KING HENRY THE EIGHTH\",\n",
    "    \"THE LIFE AND DEATH OF KING JOHN\",\n",
    "    \"THE TRAGEDY OF JULIUS CAESAR\",\n",
    "    \"THE TRAGEDY OF KING LEAR\",\n",
    "    \"LOVE’S LABOUR’S LOST\",\n",
    "    \"THE TRAGEDY OF MACBETH\",\n",
    "    \"MEASURE FOR MEASURE\",\n",
    "    \"THE MERCHANT OF VENICE\",\n",
    "    \"THE MERRY WIVES OF WINDSOR\",\n",
    "    \"A MIDSUMMER NIGHT’S DREAM\",\n",
    "    \"MUCH ADO ABOUT NOTHING\",\n",
    "    \"THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\",\n",
    "    \"PERICLES, PRINCE OF TYRE\",\n",
    "    \"KING RICHARD THE SECOND\",\n",
    "    \"KING RICHARD THE THIRD\",\n",
    "    \"THE TRAGEDY OF ROMEO AND JULIET\",\n",
    "    \"THE TAMING OF THE SHREW\",\n",
    "    \"THE TEMPEST\",\n",
    "    \"THE LIFE OF TIMON OF ATHENS\",\n",
    "    \"THE TRAGEDY OF TITUS ANDRONICUS\",\n",
    "    \"TROILUS AND CRESSIDA\",\n",
    "    \"TWELFTH NIGHT; OR, WHAT YOU WILL\",\n",
    "    \"THE TWO GENTLEMEN OF VERONA\",\n",
    "    \"THE TWO NOBLE KINSMEN\",\n",
    "    \"THE WINTER’S TALE\",\n",
    "    \"A LOVER’S COMPLAINT\",\n",
    "    \"THE PASSIONATE PILGRIM\",\n",
    "    \"THE PHOENIX AND THE TURTLE\",\n",
    "    \"THE RAPE OF LUCRECE\",\n",
    "    \"VENUS AND ADONIS\"\n",
    "\n",
    "]\n",
    "\n",
    "print(\"Number of works:\", len(shakespeare_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_works(text):\n",
    "    # Read from line 84 to skip the header and footer of Project Gutenberg text\n",
    "    lines = text.splitlines()[83:196041]\n",
    "    print(f\"Total lines after header removal: {len(lines)}\")\n",
    "    # rejoin lines into a single string for easier searching\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    # Split into works by title\n",
    "    works = []\n",
    "\n",
    "    for i in range(len(shakespeare_content)):\n",
    "        title = shakespeare_content[i]\n",
    "        next_title = shakespeare_content[i + 1] if i + 1 < len(shakespeare_content) else None\n",
    "\n",
    "        start_idx = text.find(title)\n",
    "        end_idx = text.find(next_title) if next_title else len(text)\n",
    "\n",
    "        if start_idx != -1:\n",
    "            work_text = text[start_idx:end_idx].strip()\n",
    "            works.append(work_text)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            print(f\"Extracted work {i}: {title}, length: {len(work_text)}\")\n",
    "            print(f\"Work snippet: {work_text[:50]}...\\n\")\n",
    "        else:\n",
    "            print(f\"Title '{title}' not found in text.\")\n",
    "\n",
    "    return works\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines after header removal: 195958\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 0: THE SONNETS, length: 98328\n",
      "Work snippet: THE SONNETS\n",
      "\n",
      "                    1\n",
      "\n",
      "From fairest c...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 1: ALL’S WELL THAT ENDS WELL, length: 134619\n",
      "Work snippet: ALL’S WELL THAT ENDS WELL\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 2: THE TRAGEDY OF ANTONY AND CLEOPATRA, length: 152395\n",
      "Work snippet: THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "AC...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 3: AS YOU LIKE IT, length: 127037\n",
      "Work snippet: AS YOU LIKE IT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " Scene I. An O...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 4: THE COMEDY OF ERRORS, length: 88328\n",
      "Work snippet: THE COMEDY OF ERRORS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene I. ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 5: THE TRAGEDY OF CORIOLANUS, length: 165949\n",
      "Work snippet: THE TRAGEDY OF CORIOLANUS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " Sc...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 6: CYMBELINE, length: 161233\n",
      "Work snippet: CYMBELINE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene I. Britain. Th...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 7: THE TRAGEDY OF HAMLET, PRINCE OF DENMARK, length: 177933\n",
      "Work snippet: THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conte...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 8: THE FIRST PART OF KING HENRY THE FOURTH, length: 141704\n",
      "Work snippet: THE FIRST PART OF KING HENRY THE FOURTH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conten...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 9: THE SECOND PART OF KING HENRY THE FOURTH, length: 153542\n",
      "Work snippet: THE SECOND PART OF KING HENRY THE FOURTH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conte...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 10: THE LIFE OF KING HENRY THE FIFTH, length: 153221\n",
      "Work snippet: THE LIFE OF KING HENRY THE FIFTH\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 11: THE FIRST PART OF HENRY THE SIXTH, length: 130849\n",
      "Work snippet: THE FIRST PART OF HENRY THE SIXTH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " A...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 12: THE SECOND PART OF KING HENRY THE SIXTH, length: 150498\n",
      "Work snippet: THE SECOND PART OF KING HENRY THE SIXTH\n",
      "\n",
      "Contents\n",
      "...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 13: THE THIRD PART OF KING HENRY THE SIXTH, length: 145999\n",
      "Work snippet: THE THIRD PART OF KING HENRY THE SIXTH\n",
      "\n",
      "Contents\n",
      "\n",
      "...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 14: KING HENRY THE EIGHTH, length: 143797\n",
      "Work snippet: KING HENRY THE EIGHTH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " Prolog...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 15: THE LIFE AND DEATH OF KING JOHN, length: 121394\n",
      "Work snippet: THE LIFE AND DEATH OF KING JOHN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 16: THE TRAGEDY OF JULIUS CAESAR, length: 116487\n",
      "Work snippet: THE TRAGEDY OF JULIUS CAESAR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      "...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 17: THE TRAGEDY OF KING LEAR, length: 155330\n",
      "Work snippet: THE TRAGEDY OF KING LEAR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 18: LOVE’S LABOUR’S LOST, length: 127475\n",
      "Work snippet: LOVE’S LABOUR’S LOST\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " Scene I...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 19: THE TRAGEDY OF MACBETH, length: 104449\n",
      "Work snippet: THE TRAGEDY OF MACBETH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene I...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 20: MEASURE FOR MEASURE, length: 126274\n",
      "Work snippet: MEASURE FOR MEASURE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " Scene I....\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 21: THE MERCHANT OF VENICE, length: 121269\n",
      "Work snippet: THE MERCHANT OF VENICE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene I...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 22: THE MERRY WIVES OF WINDSOR, length: 130495\n",
      "Work snippet: THE MERRY WIVES OF WINDSOR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " S...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 23: A MIDSUMMER NIGHT’S DREAM, length: 96726\n",
      "Work snippet: A MIDSUMMER NIGHT’S DREAM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scen...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 24: MUCH ADO ABOUT NOTHING, length: 122743\n",
      "Work snippet: MUCH ADO ABOUT NOTHING\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "\n",
      "Scene ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 25: THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE, length: 154214\n",
      "Work snippet: THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Con...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 26: PERICLES, PRINCE OF TYRE, length: 110393\n",
      "Work snippet: PERICLES, PRINCE OF TYRE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "ACT I\n",
      "Chor...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 27: KING RICHARD THE SECOND, length: 131066\n",
      "Work snippet: KING RICHARD THE SECOND\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " Scen...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 28: KING RICHARD THE THIRD, length: 176457\n",
      "Work snippet: KING RICHARD THE THIRD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " Scene...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 29: THE TRAGEDY OF ROMEO AND JULIET, length: 142441\n",
      "Work snippet: THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 30: THE TAMING OF THE SHREW, length: 123746\n",
      "Work snippet: THE TAMING OF THE SHREW\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "INDUCTION\n",
      " S...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 31: THE TEMPEST, length: 98743\n",
      "Work snippet: THE TEMPEST\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene I. On a ship...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 32: THE LIFE OF TIMON OF ATHENS, length: 111689\n",
      "Work snippet: THE LIFE OF TIMON OF ATHENS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 33: THE TRAGEDY OF TITUS ANDRONICUS, length: 120504\n",
      "Work snippet: THE TRAGEDY OF TITUS ANDRONICUS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 34: TROILUS AND CRESSIDA, length: 157370\n",
      "Work snippet: TROILUS AND CRESSIDA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "\n",
      "Prologue...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 35: TWELFTH NIGHT; OR, WHAT YOU WILL, length: 115453\n",
      "Work snippet: TWELFTH NIGHT; OR, WHAT YOU WILL\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 36: THE TWO GENTLEMEN OF VERONA, length: 101924\n",
      "Work snippet: THE TWO GENTLEMEN OF VERONA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 37: THE TWO NOBLE KINSMEN, length: 142416\n",
      "Work snippet: THE TWO NOBLE KINSMEN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      " ACT I\n",
      " PROLOG...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 38: THE WINTER’S TALE, length: 144017\n",
      "Work snippet: THE WINTER’S TALE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "ACT I\n",
      "Scene I. Sic...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 39: A LOVER’S COMPLAINT, length: 14359\n",
      "Work snippet: A LOVER’S COMPLAINT\n",
      "\n",
      "\n",
      "\n",
      "From off a hill whose conca...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 40: THE PASSIONATE PILGRIM, length: 17055\n",
      "Work snippet: THE PASSIONATE PILGRIM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "When my love swears...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 41: THE PHOENIX AND THE TURTLE, length: 2070\n",
      "Work snippet: THE PHOENIX AND THE TURTLE\n",
      "\n",
      "\n",
      "Let the bird of loude...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 42: THE RAPE OF LUCRECE, length: 86806\n",
      "Work snippet: THE RAPE OF LUCRECE\n",
      "\n",
      "\n",
      "TO THE RIGHT HONOURABLE\n",
      "HENR...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Extracted work 43: VENUS AND ADONIS, length: 58896\n",
      "Work snippet: VENUS AND ADONIS\n",
      "\n",
      "\n",
      "\n",
      "            _Vilia miretur vul...\n",
      "\n",
      "Number of works extracted: 44\n"
     ]
    }
   ],
   "source": [
    "shakespeare_works = extract_works(text)\n",
    "print(\"Number of works extracted:\", len(shakespeare_works))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Model Training - Incorporate the chosen dataset into our existing code framework. Train a language model that can understand the context and style of the text. (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Detail the steps taken to preprocess the text data. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recheck data - The previous step loads data as raw text and splits into 44 different Shakespeare's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows extracted: 44 \n",
      "\n",
      "Length of each work:\n",
      "Work 0 length: 98328\n",
      "Snippet of Work 0: 'THE SONNETS\\n\\n                    1\\n\\nFrom fairest creatures we desire increase,\\nT'\n",
      "Work 1 length: 134619\n",
      "Snippet of Work 1: 'ALL’S WELL THAT ENDS WELL\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Rossillon. A room in the C'\n",
      "Work 2 length: 152395\n",
      "Snippet of Work 2: 'THE TRAGEDY OF ANTONY AND CLEOPATRA\\n\\n\\nContents\\n\\nACT I\\nScene I.\\nAlexandria. A Roo'\n",
      "Work 3 length: 127037\n",
      "Snippet of Work 3: 'AS YOU LIKE IT\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. An Orchard near Oliver’s house\\n Sc'\n",
      "Work 4 length: 88328\n",
      "Snippet of Work 4: 'THE COMEDY OF ERRORS\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. A hall in the Duke’s palace\\nSc'\n",
      "Work 5 length: 165949\n",
      "Snippet of Work 5: 'THE TRAGEDY OF CORIOLANUS\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Rome. A street\\n Scene I'\n",
      "Work 6 length: 161233\n",
      "Snippet of Work 6: 'CYMBELINE\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Britain. The garden of Cymbeline’s palace'\n",
      "Work 7 length: 177933\n",
      "Snippet of Work 7: 'THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Elsinore'\n",
      "Work 8 length: 141704\n",
      "Snippet of Work 8: 'THE FIRST PART OF KING HENRY THE FOURTH\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. London. A R'\n",
      "Work 9 length: 153542\n",
      "Snippet of Work 9: 'THE SECOND PART OF KING HENRY THE FOURTH\\n\\n\\n\\n\\nContents\\n\\n INDUCTION\\n\\n ACT I\\n Scene'\n",
      "Work 10 length: 153221\n",
      "Snippet of Work 10: 'THE LIFE OF KING HENRY THE FIFTH\\n\\n\\nContents\\n\\nACT I\\nPrologue.\\nScene I. London. An'\n",
      "Work 11 length: 130849\n",
      "Snippet of Work 11: 'THE FIRST PART OF HENRY THE SIXTH\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Westminster Abb'\n",
      "Work 12 length: 150498\n",
      "Snippet of Work 12: 'THE SECOND PART OF KING HENRY THE SIXTH\\n\\nContents\\n\\n ACT I\\n Scene I. London. The '\n",
      "Work 13 length: 145999\n",
      "Snippet of Work 13: 'THE THIRD PART OF KING HENRY THE SIXTH\\n\\nContents\\n\\n ACT I\\n Scene I. London. The P'\n",
      "Work 14 length: 143797\n",
      "Snippet of Work 14: 'KING HENRY THE EIGHTH\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Prologue.\\n Scene I. London. An ante-'\n",
      "Work 15 length: 121394\n",
      "Snippet of Work 15: 'THE LIFE AND DEATH OF KING JOHN\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Northampton. A Ro'\n",
      "Work 16 length: 116487\n",
      "Snippet of Work 16: 'THE TRAGEDY OF JULIUS CAESAR\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Rome. A street\\n Scen'\n",
      "Work 17 length: 155330\n",
      "Snippet of Work 17: 'THE TRAGEDY OF KING LEAR\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. A Room of State in King Le'\n",
      "Work 18 length: 127475\n",
      "Snippet of Work 18: 'LOVE’S LABOUR’S LOST\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. The King of Navarre’s park\\n '\n",
      "Work 19 length: 104449\n",
      "Snippet of Work 19: 'THE TRAGEDY OF MACBETH\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. An open Place.\\nScene II. A C'\n",
      "Work 20 length: 126274\n",
      "Snippet of Work 20: 'MEASURE FOR MEASURE\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. An apartment in the Duke’s pa'\n",
      "Work 21 length: 121269\n",
      "Snippet of Work 21: 'THE MERCHANT OF VENICE\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Venice. A street.\\nScene II. '\n",
      "Work 22 length: 130495\n",
      "Snippet of Work 22: 'THE MERRY WIVES OF WINDSOR\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Windsor. Before Page’s'\n",
      "Work 23 length: 96726\n",
      "Snippet of Work 23: 'A MIDSUMMER NIGHT’S DREAM\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I.\\nAthens. A room in the Pal'\n",
      "Work 24 length: 122743\n",
      "Snippet of Work 24: 'MUCH ADO ABOUT NOTHING\\n\\n\\n\\n\\nContents\\n\\nACT I\\n\\nScene I.\\nBefore Leonato’s House.\\n\\nSc'\n",
      "Work 25 length: 154214\n",
      "Snippet of Work 25: 'THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Venice'\n",
      "Work 26 length: 110393\n",
      "Snippet of Work 26: 'PERICLES, PRINCE OF TYRE\\n\\n\\n\\n\\nContents\\n\\n\\nACT I\\nChorus. Before the palace of Antio'\n",
      "Work 27 length: 131066\n",
      "Snippet of Work 27: 'KING RICHARD THE SECOND\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I.   London. A Room in the p'\n",
      "Work 28 length: 176457\n",
      "Snippet of Work 28: 'KING RICHARD THE THIRD\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. London. A street\\n Scene II'\n",
      "Work 29 length: 142441\n",
      "Snippet of Work 29: 'THE TRAGEDY OF ROMEO AND JULIET\\n\\n\\n\\n\\nContents\\n\\nTHE PROLOGUE.\\n\\nACT I\\nScene I. A pu'\n",
      "Work 30 length: 123746\n",
      "Snippet of Work 30: 'THE TAMING OF THE SHREW\\n\\n\\n\\n\\nContents\\n\\nINDUCTION\\n Scene I. Before an alehouse on '\n",
      "Work 31 length: 98743\n",
      "Snippet of Work 31: 'THE TEMPEST\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. On a ship at sea; a tempestuous noise o'\n",
      "Work 32 length: 111689\n",
      "Snippet of Work 32: 'THE LIFE OF TIMON OF ATHENS\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Athens. A hall in Tim'\n",
      "Work 33 length: 120504\n",
      "Snippet of Work 33: 'THE TRAGEDY OF TITUS ANDRONICUS\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Rome. Before the '\n",
      "Work 34 length: 157370\n",
      "Snippet of Work 34: 'TROILUS AND CRESSIDA\\n\\n\\n\\n\\nContents\\n\\nACT I\\n\\nPrologue.\\nScene I.\\nTroy. Before Priam’'\n",
      "Work 35 length: 115453\n",
      "Snippet of Work 35: 'TWELFTH NIGHT; OR, WHAT YOU WILL\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. An Apartment in th'\n",
      "Work 36 length: 101924\n",
      "Snippet of Work 36: 'THE TWO GENTLEMEN OF VERONA\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n Scene I. Verona. An open place'\n",
      "Work 37 length: 142416\n",
      "Snippet of Work 37: 'THE TWO NOBLE KINSMEN\\n\\n\\n\\n\\nContents\\n\\n ACT I\\n PROLOGUE\\n Scene I. Athens. Before a '\n",
      "Work 38 length: 144017\n",
      "Snippet of Work 38: 'THE WINTER’S TALE\\n\\n\\n\\n\\nContents\\n\\nACT I\\nScene I. Sicilia. An Antechamber in Leonte'\n",
      "Work 39 length: 14359\n",
      "Snippet of Work 39: 'A LOVER’S COMPLAINT\\n\\n\\n\\nFrom off a hill whose concave womb reworded\\nA plaintful s'\n",
      "Work 40 length: 17055\n",
      "Snippet of Work 40: 'THE PASSIONATE PILGRIM\\n\\n\\n\\n\\nI\\n\\n\\nWhen my love swears that she is made of truth,\\nI '\n",
      "Work 41 length: 2070\n",
      "Snippet of Work 41: 'THE PHOENIX AND THE TURTLE\\n\\n\\nLet the bird of loudest lay,\\nOn the sole Arabian tr'\n",
      "Work 42 length: 86806\n",
      "Snippet of Work 42: 'THE RAPE OF LUCRECE\\n\\n\\nTO THE RIGHT HONOURABLE\\nHENRY WRIOTHESLEY, EARL OF SOUTHAM'\n",
      "Work 43 length: 58896\n",
      "Snippet of Work 43: 'VENUS AND ADONIS\\n\\n\\n\\n            _Vilia miretur vulgus; mihi flavus Apollo\\n      '\n"
     ]
    }
   ],
   "source": [
    "def show_work_stats(works):\n",
    "    print(\"Total rows extracted: {} \\n\".format(len(shakespeare_works)))\n",
    "    print(\"Length of each work:\")\n",
    "    \n",
    "    for i, work in enumerate(works):\n",
    "        print(f\"Work {i} length: {len(work)}\")\n",
    "        print(f\"Snippet of Work {i}: {work[:80]!r}\")\n",
    "\n",
    "show_work_stats(shakespeare_works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning and preparation\n",
    "After inspection of data downloaded from Gutenberg, multiple data cleaning steps are taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Text has been separated into each Shakespeare work while loading data\n",
    "1. Remove non-printable character\n",
    "\n",
    "2. Remove unwanted special characters except . ! ? : ' , ; and whitespace\n",
    "\n",
    "3. Add spaces around punctuation - to ensure that punctuation marks are treated as separate tokens during tokenization. This helps the language model distinguish between words and punctuation, making it easier to learn correct sentence structure and generate more accurate text. \n",
    "\n",
    "            For example, \"hello!\" becomes \"hello !\", so \"hello\" and \"!\" are separate tokens.\n",
    "\n",
    "4. Remove page numbers - idenfied as standalone numbers on lines - This has to be done before normalizing whitespaces, here the page number identification is based on single number with whitespaces in whole line.\n",
    "\n",
    "5. Normalize whitespace - will remove all types of whitespace—including newlines (\\n), tabs (\\t), and extra spaces—by replacing any sequence of whitespace characters with a single space. \n",
    "\n",
    "6. Add special tokens to denote <START> and <END> of work to help model learn boundaries and not bleed words of one work into each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_START_DELIMITER = \"<START>\"\n",
    "DOC_END_DELIMITER = \"<END>\"\n",
    "SPACE = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows extracted: 44 \n",
      "\n",
      "Length of each work:\n",
      "Work 0 length: 93664\n",
      "Snippet of Work 0: '<START> the sonnets from fairest creatures we desire increase, that thereby beau'\n",
      "Work 1 length: 134647\n",
      "Snippet of Work 1: '<START> alls well that ends well contents act i scene i . rossillon . a room in '\n",
      "Work 2 length: 153086\n",
      "Snippet of Work 2: '<START> the tragedy of antony and cleopatra contents act i scene i . alexandria '\n",
      "Work 3 length: 126026\n",
      "Snippet of Work 3: '<START> as you like it contents act i scene i . an orchard near olivers house sc'\n",
      "Work 4 length: 88663\n",
      "Snippet of Work 4: '<START> the comedy of errors contents act i scene i . a hall in the dukes palace'\n",
      "Work 5 length: 166483\n",
      "Snippet of Work 5: '<START> the tragedy of coriolanus contents act i scene i . rome . a street scene'\n",
      "Work 6 length: 160794\n",
      "Snippet of Work 6: '<START> cymbeline contents act i scene i . britain . the garden of cymbelines pa'\n",
      "Work 7 length: 177755\n",
      "Snippet of Work 7: '<START> the tragedy of hamlet, prince of denmark contents act i scene i . elsino'\n",
      "Work 8 length: 142231\n",
      "Snippet of Work 8: '<START> the first part of king henry the fourth contents act i scene i . london '\n",
      "Work 9 length: 154223\n",
      "Snippet of Work 9: '<START> the second part of king henry the fourth contents induction act i scene '\n",
      "Work 10 length: 153251\n",
      "Snippet of Work 10: '<START> the life of king henry the fifth contents act i prologue . scene i . lon'\n",
      "Work 11 length: 131011\n",
      "Snippet of Work 11: '<START> the first part of henry the sixth contents act i scene i . westminster a'\n",
      "Work 12 length: 150940\n",
      "Snippet of Work 12: '<START> the second part of king henry the sixth contents act i scene i . london '\n",
      "Work 13 length: 146391\n",
      "Snippet of Work 13: '<START> the third part of king henry the sixth contents act i scene i . london .'\n",
      "Work 14 length: 144149\n",
      "Snippet of Work 14: '<START> king henry the eighth contents act i prologue . scene i . london . an an'\n",
      "Work 15 length: 121741\n",
      "Snippet of Work 15: '<START> the life and death of king john contents act i scene i . northampton . a'\n",
      "Work 16 length: 116872\n",
      "Snippet of Work 16: '<START> the tragedy of julius caesar contents act i scene i . rome . a street sc'\n",
      "Work 17 length: 154951\n",
      "Snippet of Work 17: '<START> the tragedy of king lear contents act i scene i . a room of state in kin'\n",
      "Work 18 length: 127542\n",
      "Snippet of Work 18: '<START> loves labours lost contents act i scene i . the king of navarres park sc'\n",
      "Work 19 length: 104362\n",
      "Snippet of Work 19: '<START> the tragedy of macbeth contents act i scene i . an open place . scene ii'\n",
      "Work 20 length: 127051\n",
      "Snippet of Work 20: '<START> measure for measure contents act i scene i . an apartment in the dukes p'\n",
      "Work 21 length: 121461\n",
      "Snippet of Work 21: '<START> the merchant of venice contents act i scene i . venice . a street . scen'\n",
      "Work 22 length: 131035\n",
      "Snippet of Work 22: '<START> the merry wives of windsor contents act i scene i . windsor . before pag'\n",
      "Work 23 length: 95491\n",
      "Snippet of Work 23: '<START> a midsummer nights dream contents act i scene i . athens . a room in the'\n",
      "Work 24 length: 123420\n",
      "Snippet of Work 24: '<START> much ado about nothing contents act i scene i . before leonatos house . '\n",
      "Work 25 length: 154877\n",
      "Snippet of Work 25: '<START> the tragedy of othello, the moor of venice contents act i scene i . veni'\n",
      "Work 26 length: 110187\n",
      "Snippet of Work 26: '<START> pericles, prince of tyre contents act i chorus . before the palace of an'\n",
      "Work 27 length: 131483\n",
      "Snippet of Work 27: '<START> king richard the second contents act i scene i . london . a room in the '\n",
      "Work 28 length: 177233\n",
      "Snippet of Work 28: '<START> king richard the third contents act i scene i . london . a street scene '\n",
      "Work 29 length: 142737\n",
      "Snippet of Work 29: '<START> the tragedy of romeo and juliet contents the prologue . act i scene i . '\n",
      "Work 30 length: 123813\n",
      "Snippet of Work 30: '<START> the taming of the shrew contents induction scene i . before an alehouse '\n",
      "Work 31 length: 98508\n",
      "Snippet of Work 31: '<START> the tempest contents act i scene i . on a ship at sea; a tempestuous noi'\n",
      "Work 32 length: 112090\n",
      "Snippet of Work 32: '<START> the life of timon of athens contents act i scene i . athens . a hall in '\n",
      "Work 33 length: 120821\n",
      "Snippet of Work 33: '<START> the tragedy of titus andronicus contents act i scene i . rome . before t'\n",
      "Work 34 length: 157502\n",
      "Snippet of Work 34: '<START> troilus and cressida contents act i prologue . scene i . troy . before p'\n",
      "Work 35 length: 115371\n",
      "Snippet of Work 35: '<START> twelfth night; or, what you will contents act i scene i . an apartment i'\n",
      "Work 36 length: 102330\n",
      "Snippet of Work 36: '<START> the two gentlemen of verona contents act i scene i . verona . an open pl'\n",
      "Work 37 length: 142494\n",
      "Snippet of Work 37: '<START> the two noble kinsmen contents act i prologue scene i . athens . before '\n",
      "Work 38 length: 143756\n",
      "Snippet of Work 38: '<START> the winters tale contents act i scene i . sicilia . an antechamber in le'\n",
      "Work 39 length: 14196\n",
      "Snippet of Work 39: '<START> a lovers complaint from off a hill whose concave womb reworded a plaintf'\n",
      "Work 40 length: 16586\n",
      "Snippet of Work 40: '<START> the passionate pilgrim i when my love swears that she is made of truth, '\n",
      "Work 41 length: 2056\n",
      "Snippet of Work 41: '<START> the phoenix and the turtle let the bird of loudest lay, on the sole arab'\n",
      "Work 42 length: 84326\n",
      "Snippet of Work 42: '<START> the rape of lucrece to the right honourable henry wriothesley, earl of s'\n",
      "Work 43 length: 55134\n",
      "Snippet of Work 43: '<START> venus and adonis vilia miretur vulgus; mihi flavus apollo pocula castali'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_data(works):\n",
    "    cleaned_works = []\n",
    "    for work in works:\n",
    "        # Lowercase\n",
    "        work = work.lower()\n",
    "\n",
    "        # Remove non-ASCII , non-printable data\n",
    "        work = re.sub(r'[^\\x00-\\x7F]+', '', work)\n",
    "        # Remove unwanted special characters except . ! ? : ' , ; and whitespace\n",
    "        work = re.sub(r\"[^a-z0-9\\.\\!\\?\\:\\'\\,\\;\\s]\", '', work)\n",
    "        # Add spaces around punctuation\n",
    "        work = re.sub(r'([\\.\\!\\?])', r' \\1 ', work)\n",
    "        # remove page numbers - idenfied as standalone numbers on lines\n",
    "        work = re.sub(r'^\\s*\\d+\\s*$', '', work, flags=re.MULTILINE)\n",
    "        # Normalize whitespace\n",
    "        work = re.sub(r'\\s+', ' ', work).strip()\n",
    "\n",
    "        \n",
    "\n",
    "        # Add special tokens to denote <START> and <END> of work to help model learn boundaries \n",
    "        # and not bleed words of one work into each other\n",
    "        cleaned_works.append(DOC_START_DELIMITER + SPACE + work + SPACE + DOC_END_DELIMITER)\n",
    "    return cleaned_works\n",
    "\n",
    "shakespeare_works_clean = clean_data(shakespeare_works)\n",
    "\n",
    "show_work_stats(shakespeare_works_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Hugging face Dataset :\n",
    "\n",
    "Structure:\n",
    "\n",
    "- A Dataset is like a table (similar to a pandas DataFrame), where each row is a data sample and each column is a feature (e.g., \"text\", \"label\").\n",
    "- It supports multiple columns, various data types, and can be split into train/validation/test sets using a DatasetDict.\n",
    "\n",
    "Usage: \n",
    "\n",
    "```sh\n",
    "# load data sets from Hugging face hub or from local files\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")  # Loads the IMDB reviews dataset\n",
    "\n",
    "# create from python objects eg. list or array\n",
    "from datasets import Dataset\n",
    "data = [{\"text\": \"hello\", \"label\": 0}, {\"text\": \"world\", \"label\": 1}]\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# accessing data\n",
    "print(dataset[0])  # {'text': 'hello', 'label': 0}\n",
    "\n",
    "# processing - use map functions, filter, shuffle, and split datasets efficiently.\n",
    "dataset = dataset.map(lambda x: {\"text\": x[\"text\"].upper()})\n",
    "\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Handles large datasets efficiently (memory-mapped, streaming).\n",
    "- Integrates seamlessly with Hugging Face Transformers for model training.\n",
    "- Supports easy preprocessing, tokenization, and batching.\n",
    "- Built-in support for dataset splits, shuffling, and filtering.\n",
    "- Can load from many formats (CSV, JSON, text, etc.) and the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def list_to_dataset(data_list):\n",
    "   return Dataset.from_list([{\"text\": item} for item in data_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 44\n",
      "})\n",
      "<START> the sonnets from fairest creatures we desire increase, that thereby beautys rose might never die, but as the riper should by time decease, his tender heir might bear his memory: but thou contr\n"
     ]
    }
   ],
   "source": [
    "# Convert shakespeare_works_clean to a Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "sp_datasets = Dataset.from_list([{\"text\": work} for work in shakespeare_works_clean])\n",
    "print(sp_datasets)\n",
    "print(sp_datasets[0]['text'][:200])  # Print the first 200 characters of the first entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_test = sp_datasets.train_test_split(test_size=0.2)\n",
    "\n",
    "# 10% test set and 10% validation set\n",
    "train_test_valid = train_test['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test_valid['test'],\n",
    "    'validation': train_test_valid['train']})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> the tragedy of king lear contents act i scene i . a room of state in kin\n",
      "<START> the tragedy of julius caesar contents act i scene i . rome . a street sc\n",
      "<START> the life of king henry the fifth contents act i prologue . scene i . lon\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['text'][:80])\n",
    "print(dataset['validation'][0]['text'][:80]) \n",
    "print(dataset['test'][0]['text'][:80]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d9548f23a64c6989b4dda733aa71e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666ef9a931d148fda2296c2b4434d20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ceda5245a634f198d4882be385c33c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exact copy of torchtext's basic_english tokenizer\n",
    "# Source: https://github.com/pytorch/text/blob/main/torchtext/data/utils.py\n",
    "\n",
    "_patterns = [r\"\\'\", r\"\\\"\", r\"\\.\", r\"<br \\/>\", r\",\", r\"\\(\", r\"\\)\", r\"\\!\", r\"\\?\", r\"\\;\", r\"\\:\", r\"\\s+\"]\n",
    "_replacements = [\" '  \", \"\", \" . \", \" \", \" , \", \" ( \", \" ) \", \" ! \", \" ? \", \" \", \" \", \" \"]\n",
    "_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n",
    "\n",
    "def _basic_english_normalize(line):\n",
    "    line = line.lower()\n",
    "    for pattern_re, replaced_str in _patterns_dict:\n",
    "        line = pattern_re.sub(replaced_str, line)\n",
    "    return line.split()\n",
    "\n",
    "def basic_english_tokenizer(text):\n",
    "    \"\"\"Tokenizer matching torchtext's basic_english implementation\"\"\"\n",
    "    return _basic_english_normalize(text)\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': basic_english_tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'the', 'tragedy', 'of', 'king', 'lear', 'contents', 'act', 'i', 'scene', 'i', '.', 'a', 'room', 'of', 'state', 'in', 'king', 'lears', 'palace']\n",
      "['<start>', 'the', 'tragedy', 'of', 'julius', 'caesar', 'contents', 'act', 'i', 'scene', 'i', '.', 'rome', '.', 'a', 'street', 'scene', 'ii', '.', 'the']\n",
      "['<start>', 'the', 'life', 'of', 'king', 'henry', 'the', 'fifth', 'contents', 'act', 'i', 'prologue', '.', 'scene', 'i', '.', 'london', '.', 'an', 'antechamber']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][0]['tokens'][:20])\n",
    "print(tokenized_dataset['validation'][0]['tokens'][:20]) \n",
    "print(tokenized_dataset['test'][0]['tokens'][:20]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numericalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also add `unk` to handle missing vocab and `eos` to identify the end of sentence.\n",
    "\n",
    "This is a common and recommended practice in NLP. Limiting the vocabulary to words that appear at least a few times (e.g., 2 or 3) helps reduce memory usage and model complexity, while special tokens like unk and eos are standard for handling unknown words and marking sequence boundaries. This approach is widely used in language modeling and text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "END_OF_SENTENCE_TOKEN = \"<eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Vocab class to replace torchtext.vocab\n",
    "class Vocab:\n",
    "    def __init__(self, counter, min_freq=1, specials=None):\n",
    "        self.itos = []  # index to string\n",
    "        self.stoi = {}  # string to index\n",
    "        self.default_index = 0\n",
    "        \n",
    "        # Add special tokens first\n",
    "        if specials:\n",
    "            for token in specials:\n",
    "                self._add_token(token)\n",
    "        \n",
    "        # Add tokens that meet min_freq threshold\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq:\n",
    "                if token not in self.stoi:\n",
    "                    self._add_token(token)\n",
    "    \n",
    "    def _add_token(self, token):\n",
    "        if token not in self.stoi:\n",
    "            self.stoi[token] = len(self.itos)\n",
    "            self.itos.append(token)\n",
    "    \n",
    "    def set_default_index(self, index):\n",
    "        self.default_index = index\n",
    "    \n",
    "    def get_itos(self):\n",
    "        return self.itos\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.default_index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "# Build vocabulary from tokenized data\n",
    "counter = Counter()\n",
    "for tokens in tokenized_dataset['train']['tokens']:\n",
    "    counter.update(tokens)\n",
    "\n",
    "vocab = Vocab(counter, min_freq=3, specials=[UNKNOWN_TOKEN, END_OF_SENTENCE_TOKEN])\n",
    "vocab.set_default_index(vocab[UNKNOWN_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 11372\n",
      "['<unk>', '<eos>', ',', '.', 'the', 'and', 'i', 'to', 'of', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of vocab: {len(vocab)}\")                         \n",
    "print(vocab.get_itos()[:10])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n",
    "    return data #[batch size, seq len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 7373])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Describe the model architecture and the training process. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"class/figures/LM.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        # Fix: use .data.uniform_() instead of replacing the tensor\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param.data.uniform_(-init_range_other, init_range_other)\n",
    "            elif 'bias' in name:\n",
    "                param.data.zero_()\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "# A dropout rate of 0.65 means that during training, 65% of the neurons in the dropout layers \n",
    "# are randomly set to zero at each update, which helps prevent overfitting and improves generalization.\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 40,094,828 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    # tqdm for progress bar\n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../model'\n",
    "model_filename = device.type + \"_a2-lstm_lm.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainiing using device: cpu  for  10  epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Perplexity: 454.039\n",
      "\tValid Perplexity: 222.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Perplexity: 260.505\n",
      "\tValid Perplexity: 171.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tTrain Perplexity: 199.797\n",
      "\tValid Perplexity: 146.579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04\n",
      "\tTrain Perplexity: 170.912\n",
      "\tValid Perplexity: 132.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05\n",
      "\tTrain Perplexity: 153.192\n",
      "\tValid Perplexity: 122.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06\n",
      "\tTrain Perplexity: 141.031\n",
      "\tValid Perplexity: 117.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07\n",
      "\tTrain Perplexity: 130.901\n",
      "\tValid Perplexity: 113.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08\n",
      "\tTrain Perplexity: 122.639\n",
      "\tValid Perplexity: 109.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09\n",
      "\tTrain Perplexity: 115.252\n",
      "\tValid Perplexity: 106.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "\tTrain Perplexity: 109.006\n",
      "\tValid Perplexity: 103.996\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10 if device.type == 'cpu' else 50\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "print(\"Trainiing using device:\", device.type, \" for \", n_epochs, \" epochs\")\n",
    "\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'{model_path}/{model_filename}')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of GPU vs CPU performance on training model\n",
    "\n",
    "GPU training Perplexity:\n",
    "```sh\n",
    "Epoch: 50\n",
    "\tTrain Perplexity: 79.642\n",
    "\tValid Perplexity: 147.209\n",
    "```\n",
    "\n",
    "CPU training Perpexity:\n",
    "\n",
    "```sh\n",
    "Epoch: 10\n",
    "\tTrain Perplexity: 109.006\n",
    "\tValid Perplexity: 103.996\n",
    "```\n",
    "CPU training completed in 16m 43.3s\n",
    "\n",
    "\n",
    "GPU is more than 5x faster to complete the training for 50 epoch. However, the GPU training perplexity is higher even with 50 epoch, where as CPU perplexity is ~100 by 10th epoch. I will be using model trained by CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model and evaluating on test set from location ../model/cpu_a2-lstm_lm.pt\n",
      "Test Perplexity: 140.969\n"
     ]
    }
   ],
   "source": [
    "model_full_path = f'{model_path}/{model_filename}'\n",
    "print(\"Loading best model and evaluating on test set from location \" + model_full_path)\n",
    "model.load_state_dict(torch.load(model_full_path,  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save and Load Model (Pickling)\n",
    "\n",
    "PyTorch provides two ways to save models:\n",
    "1. **Save state_dict (Recommended)** - Only saves weights, need model class to load\n",
    "2. **Save entire model** - Uses pickle, saves everything but less portable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = \"../model\"\n",
    "checkout_filename = \"lstm_lm_checkpoint.pt\"\n",
    "checkpoint = torch.load(f'{checkpoint_path}/{checkout_filename}', map_location=device)\n",
    "\n",
    "# Recreate model with saved hyperparameters\n",
    "loaded_model = LSTMLanguageModel(\n",
    "    checkpoint['vocab_size'],\n",
    "    checkpoint['emb_dim'],\n",
    "    checkpoint['hid_dim'],\n",
    "    checkpoint['num_layers'],\n",
    "    checkpoint['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "# Load weights\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded from checkpoint!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle vocab\n",
    "import pickle\n",
    "\n",
    "vocab_filename = \"a2_vocab_lm.pkl\"\n",
    "\n",
    "with open(f'{model_path}/{vocab_filename}', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(prompt_str):\n",
    "    max_seq_len = 30\n",
    "    seed = 0\n",
    "\n",
    "    #smaller the temperature, more diverse tokens but comes \n",
    "    #with a tradeoff of less-make-sense sentence\n",
    "    temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "    for temperature in temperatures:\n",
    "        generation = generate(prompt_str, max_seq_len, temperature, model, basic_english_tokenizer, \n",
    "                            vocab, device, seed)\n",
    "        print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "to be , or not to be , that is the question of your voices . i know you , for the more of a match , and the king , to make my best part to the ground , to say\n",
      "\n",
      "0.7\n",
      "to be , or not to be , that is the question of your voices . yet ill fear her . what now , go from me . exit . act v scene i . the same . the same camp near\n",
      "\n",
      "0.75\n",
      "to be , or not to be , that is the question of your voices . yet were you gone , being now alive to from me . he doth make his language . but and tell the day o th towns\n",
      "\n",
      "0.8\n",
      "to be , or not to be , that is the question of your voices . yet were you gone , being now alive to from me . he doth always . besides , but and tell the day o th towns\n",
      "\n",
      "1.0\n",
      "to be , or not to be , that is the question of your voices . yet were you gone for being now alive to from me . he doth always . besides , condition and divorce the dead two maid !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1 : Prompts exactly as Shakespeare's words\n",
    "prompt(\"To be, or not to be, that is the question:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "the sonnet opens with a prologue that sets the scene , and the better of her . go , come . exit . enter falstaff . tranio . my lord , i do beseech you , sir , i will\n",
      "\n",
      "0.7\n",
      "the sonnet opens with a prologue that sets the scene , and the one of her . what now , go from me . jaquenetta . the king is a word . palamon . o , sir , you will\n",
      "\n",
      "0.75\n",
      "the sonnet opens with a prologue that sets the scene queen and the prince of anjou , attended to her . go from me . exit . act v scene i . rome . a room in the countesss palace\n",
      "\n",
      "0.8\n",
      "the sonnet opens with a prologue that sets the scene queen and the prince of anjou , attended to her . go from me . exit . act v scene i . the same . the same camp near dover\n",
      "\n",
      "1.0\n",
      "the sonnet opens with a prologue that sets the scene dull and the are of one . reenter gentlemen . enter brabantio . posthumus . it is stirrd to the tower . prince . tell the tribunes , sir .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2 : Another Prompts exactly as Shakespeare's words\n",
    "prompt(\"The sonnet opens with a prologue that sets the scene:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "<unk> is a man . i know you , for the more of a match , and the other , and by the two of heaven , the next cock , the\n",
      "\n",
      "0.7\n",
      "<unk> is a pair of men . he is himself to her . go from me . exit . act v scene i . the same . the same camp near dover\n",
      "\n",
      "0.75\n",
      "<unk> is a pair of men . romeo . ill warrant thee more . posthumus . it is a very . besides , but and tell the day o th towns of\n",
      "\n",
      "0.8\n",
      "<unk> is a pair of acheron . he kills himself . enter the lieutenant from france . jaquenetta . my lord , i shall be best by the duke of our towns\n",
      "\n",
      "1.0\n",
      "<unk> is returned and made him to see . reenter gentlemen . enter the lieutenant from antonio . jaquenetta . sirrah . clarence . trinculo , and tell the tribunes . two\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 3 : Prompts with missing token\n",
    "prompt(\"Frankenstein is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "the meaning of life is returned and made him to the king . enter the king . king . he doth the king , and i will see him . king . o , you\n",
      "\n",
      "0.7\n",
      "the meaning of life is returned and made him to the better . enter the king . antonio . he doth the king . trinculo . and tell the duke of battle , and ,\n",
      "\n",
      "0.75\n",
      "the meaning of life is returned and made him to see . reenter gentlemen . enter the lieutenant . antonio . most wise , ophelia . i do not think it . poins . o\n",
      "\n",
      "0.8\n",
      "the meaning of life is returned and made him to see . reenter gentlemen . enter the lieutenant . antonio . most wise , ophelia . i do not think it . poins . o\n",
      "\n",
      "1.0\n",
      "the meaning of life is returned and made him to see . reenter gentlemen . enter brabantio . posthumus . it is stirrd to the tower . prince . tell the tribunes , sir .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 4 : Prompts for new creation\n",
    "prompt(\"The meaning of life is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Text Generation - Web Application Development - Develop a simple web application thatdemonstrates the capabilities of your language model. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) The application should include an input box where users can type in a text prompt.\n",
    "2) Based on the input, the model should generate and display a continuation of the text. For example,\n",
    "if the input is ”Harry Potter is”, the model might generate ”a wizard in the world of Hogwarts”.\n",
    "3) Provide documentation on how the web application interfaces with the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\">ANSWER:</font>\n",
    "\n",
    "Multiple steps are carried out to develop the web application. Flask - general purpose web framework is used for this assignment. \n",
    "\n",
    "Web app folder : `A2/app`\n",
    "\n",
    "Python file list:\n",
    "1. `app.py` - main python code to run the site. Use `uv run app.py` to start web app.\n",
    "2. `lstm.py` - LSTMLanguageModel class \n",
    "3. `tokenizer.py` - helps to tokenize the text. `torchtext.data.util.tokenizer` is incompatible and deprecated with python v3.13.x. To fix this a copy of code maintained in codebase.\n",
    "4. `vocab.py` - Vocab class \n",
    "\n",
    "As per recommendation, instead of saving big size model, a checkpoint is saved and later loaded in application to build LSTM model with saved hyperparameters. Code to load the model is contained in `app.py`\n",
    "\n",
    "Check [README](../README.md) for more details and screenshots\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
