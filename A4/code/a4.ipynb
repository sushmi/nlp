{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3c9b74",
   "metadata": {},
   "source": [
    "# A4: Do you AGREE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551a234",
   "metadata": {},
   "source": [
    "This assignment will guide you in training a pre-trained model like BERT from scratch, focusing onleveraging text embeddings to capture semantic similarity. \n",
    "\n",
    "Additionally, we will explore how to adapt the loss function for tasks like Natural Language Inference (NLI) to enhance the model’s ability to understand semantic relationships between texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45d9ef",
   "metadata": {},
   "source": [
    "#### Step 0: Prepare Environment - Import Libraries and select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e37322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import datasets, math, re, random, time\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7389429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mimimum required torch version for MPS support \"1.12+\"\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8931caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub login token\n",
    "# Make sure to set the HF_TOKEN environment variable in your .env file with your Hugging Face token\n",
    "import os\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde7b574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# universal device selection: use gpu if available, else cpu\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear CUDA cache to free up memory \n",
    "        return torch.device(\"cuda\")      # NVIDIA GPU\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()  # Clear MPS cache to avoid memory issues\n",
    "        return torch.device(\"mps\")       # Apple Silicon GPU\n",
    "    else:\n",
    "        torch.empty_cache()  # Clear CPU cache to free up memory\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c47e7",
   "metadata": {},
   "source": [
    "## Task 1. Training BERT from Scratch - Based on Masked Language Model/BERT-update.ipynb, modify as follows: (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b45515",
   "metadata": {},
   "source": [
    "### 1.1) Implement Bidirectional Encoder Representations from Transformers (BERT) from scratch, following the concepts learned in class.\n",
    "\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca263a1c",
   "metadata": {},
   "source": [
    "#### Step 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f9a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub login token\n",
    "# Make sure to set the HF_TOKEN environment variable in your .env file with your Hugging Face token\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f59994",
   "metadata": {},
   "source": [
    "Using dataset from [Salesforce wikitext](https://huggingface.co/datasets/Salesforce/wikitext)\n",
    "\n",
    "Info about Dataset:\n",
    "wikitext-103-raw-v1\n",
    "```sh\n",
    "    Size of downloaded dataset files: 191.98 MB\n",
    "    Size of the generated dataset: 549.42 MB\n",
    "    Total amount of disk used: 741.41 MB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f76aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1801350\n",
      "Validation set size: 3760\n",
      "Test set size: 4358\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# data folder are not uploaded to Github.\n",
    "_DATA_PATH = \"../data/wikitext-103\"\n",
    "_DATA_FILENAME = os.path.join(_DATA_PATH, \"wikitext-103-train.arrow\")\n",
    "os.makedirs(_DATA_PATH, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(_DATA_FILENAME):\n",
    "    # Download and save to local folder\n",
    "    dataset_train = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\", cache_dir=_DATA_PATH)\n",
    "    dataset_valid = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\", cache_dir=_DATA_PATH)\n",
    "    dataset_test = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\", cache_dir=_DATA_PATH)\n",
    "\n",
    "else:\n",
    "    # Load from local Parquet file\n",
    "    from datasets import Dataset\n",
    "    dataset_train = Dataset.from_parquet(_DATA_FILENAME)\n",
    "    dataset_valid = Dataset.from_parquet(os.path.join(_DATA_PATH, \"wikitext-103-validation.arrow\"))\n",
    "    dataset_test = Dataset.from_parquet(os.path.join(_DATA_PATH, \"wikitext-103-test.arrow\"))\n",
    "    print(\"Loaded datasets from local Parquet files.\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset_train)}\")\n",
    "print(f\"Validation set size: {len(dataset_valid)}\")\n",
    "print(f\"Test set size: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0350b",
   "metadata": {},
   "source": [
    "Check dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76001bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48eef587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4358\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a55249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3760\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9221afc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Va',\n",
       " ' The game began development in 2010 , carrying over a large portion of the work ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 entries in the dataset \n",
    "# with only the first 80 characters of the text for brevity\n",
    "[text[:80] for text in dataset_train[:5]['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db211b",
   "metadata": {},
   "source": [
    "#### Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f984fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [re.sub(\"[.,!?\\\\-]\", '', t.lower()) for t in dataset_train[\"text\"] if t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1982a",
   "metadata": {},
   "source": [
    "#### Step 3: Tokenization \n",
    "\n",
    "using spaCy tokenizer or NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d44a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in raw text: 538294333\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\".join(dataset_train['text'])\n",
    "print(f\"Total characters in raw text: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e5ed9",
   "metadata": {},
   "source": [
    "Too big dataset. It will fail spacy max_lenght limit validation. The validation exists to prevent memory allocation error. Don't concatenate, but use for loop on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9be5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = Valkyria Chronicles III = \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[\"text\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f62bc",
   "metadata": {},
   "source": [
    "Since spaCy is slow, Using NLTK.\n",
    "\n",
    "The command nltk.download('punkt') downloads the \"punkt\" tokenizer models for NLTK. \"Punkt\" is a pre-trained model used by NLTK for sentence splitting and word tokenization in English and other languages. Without downloading \"punkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f94e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ../models/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to ../models/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'valkyria', 'chronicles', 'iii', '=']\n",
      "['senjō', 'no', 'valkyria', '3', ':', 'unrecorded', 'chronicles', '(', 'japanese', ':', '戦場のヴァルキュリア3', 'lit', 'valkyria', 'of', 'the', 'battlefield', '3', ')', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'is', 'a', 'tactical', 'role', '@', '@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'mediavision', 'for', 'the', 'playstation', 'portable', 'released', 'in', 'january', '2011', 'in', 'japan', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', 'employing', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@', '@', 'time', 'gameplay', 'as', 'its', 'predecessors', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '``', 'nameless', '``', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '``', 'calamaty', 'raven', '``']\n",
      "['the', 'game', 'began', 'development', 'in', '2010', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', 'while', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series', 'it', 'also', 'underwent', 'multiple', 'adjustments', 'such', 'as', 'making', 'the', 'game', 'more', 'forgiving', 'for', 'series', 'newcomers', 'character', 'designer', 'raita', 'honjou', 'and', 'composer', 'hitoshi', 'sakimoto', 'both', 'returned', 'from', 'previous', 'entries', 'along', 'with', 'valkyria', 'chronicles', 'ii', 'director', 'takeshi', 'ozawa', 'a', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', 'the', 'game', \"'s\", 'opening', 'theme', 'was', 'sung', 'by', 'may', \"'n\"]\n",
      "['it', 'met', 'with', 'positive', 'sales', 'in', 'japan', 'and', 'was', 'praised', 'by', 'both', 'japanese', 'and', 'western', 'critics', 'after', 'release', 'it', 'received', 'downloadable', 'content', 'along', 'with', 'an', 'expanded', 'edition', 'in', 'november', 'of', 'that', 'year', 'it', 'was', 'also', 'adapted', 'into', 'manga', 'and', 'an', 'original', 'video', 'animation', 'series', 'due', 'to', 'low', 'sales', 'of', 'valkyria', 'chronicles', 'ii', 'valkyria', 'chronicles', 'iii', 'was', 'not', 'localized', 'but', 'a', 'fan', 'translation', 'compatible', 'with', 'the', 'game', \"'s\", 'expanded', 'edition', 'was', 'released', 'in', '2014', 'mediavision', 'would', 'return', 'to', 'the', 'franchise', 'with', 'the', 'development', 'of', 'valkyria', ':', 'azure', 'revolution', 'for', 'the', 'playstation', '4']\n",
      "['=', '=', 'gameplay', '=', '=']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "_DOWNLOAD_DIR = \"../models/nltk_data\"\n",
    "os.makedirs(_DOWNLOAD_DIR, exist_ok=True)\n",
    "nltk.download('punkt', download_dir=_DOWNLOAD_DIR)\n",
    "nltk.download('punkt_tab', download_dir=_DOWNLOAD_DIR)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.data.path.append(_DOWNLOAD_DIR)\n",
    "tokenized_texts = [word_tokenize(text) for text in sentences]\n",
    "\n",
    "# Example: print the first 5 tokenized samples\n",
    "for tokens in tokenized_texts[:5]:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d79bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1165029"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d795b64",
   "metadata": {},
   "source": [
    "#### Step 4: Build Vocabulary - Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed3411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making vocabs - numericalization\n",
    "_PAD_TOKEN = '[PAD]'\n",
    "_CLS_TOKEN = '[CLS]'\n",
    "_SEP_TOKEN = '[SEP]'\n",
    "_MASK_TOKEN = '[MASK]'\n",
    "word_list = list(set(\" \".join([sent for sublist in tokenized_texts for sent in sublist]).split()))\n",
    "word2id   = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fc8efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word2id\n",
    "for i, w in enumerate(word_list):\n",
    "    word2id[w] = i + 4  # reserve the first 0-3 for CLS, PAD\n",
    "\n",
    "# Create id2word and vocab_size after word2id is complete\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "# Numericalize tokenized_texts\n",
    "token_list = [\n",
    "    [word2id[word] for word in sentence if word in word2id]\n",
    "    for sentence in tokenized_texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23aa18d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1165029"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "855d19b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 token IDs: [290398, 86317, 55900, 59127, 290398]...\n",
      "Sample 2 token IDs: [389713, 385399, 86317, 17796, 467181, 415568, 55900, 171827, 418081, 467181]...\n",
      "Sample 3 token IDs: [252381, 204127, 172171, 505155, 371455, 213713, 242738, 160963, 65705, 412394]...\n",
      "Sample 4 token IDs: [150193, 491742, 33825, 141503, 164720, 371455, 386039, 83500, 338935, 353199]...\n",
      "Sample 5 token IDs: [290398, 290398, 502282, 290398, 290398]...\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(token_list[:5]):\n",
    "    print(f\"Sample {i+1} token IDs: {token[:10]}...\")  # Print first 10 token IDs for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64c05f",
   "metadata": {},
   "source": [
    "\n",
    "### 2) Train the model on a suitable dataset. Ensure to source this dataset from reputable public databases or repositories. It is imperative to give proper credit to the dataset source in your documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178b3bc",
   "metadata": {},
   "source": [
    "#### Step 5: Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb7e27",
   "metadata": {},
   "source": [
    "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
    "\n",
    "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
    "\n",
    "2. **Segment embedding**\n",
    "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
    "\n",
    "3. **Masking**\n",
    "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
    "\n",
    "4. **Padding**\n",
    "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
    "\n",
    "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c9fda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5 #even though it does not reach 15% yet....maybe you can set this threshold\n",
    "max_len    = 1000 #maximum length that my transformer will accept.....all sentence will be padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b26a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, randrange\n",
    "from random import shuffle\n",
    "from random import random\n",
    "\n",
    "\n",
    "\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        \n",
    "        #randomly choose two sentence\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        \n",
    "        #1. token embedding - add CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "        \n",
    "        #2. segment embedding - which sentence is 0 and 1\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "        #3 masking\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] \n",
    "                                 and token != word2id['[SEP]']]\n",
    "        shuffle(candidates_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.1:  #10% replace with random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word2id[id2word[index]]\n",
    "            elif random() < 0.8:  #80 replace with [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        \n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "        \n",
    "        #6. check whether is positive or negative\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "        \n",
    "    return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab760b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c5696cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77bc9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd4e809a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1000]),\n",
       " torch.Size([6, 1000]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6, 5]),\n",
       " tensor([0, 0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "186acf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[159136,  83500, 353630, 170062, 252381],\n",
       "        [171827, 445011, 353630, 317874, 172171],\n",
       "        [290398, 143500, 290398,      0,      0],\n",
       "        [311278, 457581,  80590, 345657,  80539],\n",
       "        [510825,  14529, 510825, 352329, 388523],\n",
       "        [ 83500, 296795, 252381, 344607, 325249]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79222c25",
   "metadata": {},
   "source": [
    "#### Step 6. Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52993252",
   "metadata": {},
   "source": [
    "\n",
    "Recall that BERT only uses the encoder.\n",
    "\n",
    "BERT has the following components:\n",
    "\n",
    "- Embedding layers\n",
    "- Attention Mask\n",
    "- Encoder layer\n",
    "- Multi-head attention\n",
    "- Scaled dot product attention\n",
    "- Position-wise feed-forward network\n",
    "- BERT (assembling all the components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06ca61",
   "metadata": {},
   "source": [
    "##### 6.1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae014de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.device = device\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2460af",
   "metadata": {},
   "source": [
    "##### 6.2 Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1093ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, device=None):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ceedd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd77f3",
   "metadata": {},
   "source": [
    "##### 6.3 Encoder\n",
    "\n",
    "The encoder has two main components: \n",
    "\n",
    "- Multi-head Attention\n",
    "- Position-wise feed-forward network\n",
    "\n",
    "First let's make the wrapper called `EncoderLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e60a4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k)\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46167a91",
   "metadata": {},
   "source": [
    "Let's define the scaled dot attention, to be used inside the multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac33aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a453c5",
   "metadata": {},
   "source": [
    "Let's define the parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddf6d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7c98a",
   "metadata": {},
   "source": [
    "Here is the Multiheadattention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a21b4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k  # d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_k, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "\n",
    "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        output = self.linear(context)\n",
    "        return self.norm(output + residual), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8194d8",
   "metadata": {},
   "source": [
    "Here is the PoswiseFeedForwardNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7302a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe590f9a",
   "metadata": {},
   "source": [
    "##### 6.4 Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ae9ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a4378",
   "metadata": {},
   "source": [
    "#### Step 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fff20801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 loss = 118.843636\n",
      "Epoch: 05 loss = 138.800888\n",
      "Epoch: 10 loss = 57.787827\n",
      "Epoch: 15 loss = 34.719833\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epoch = 20 #500\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ").to(device) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos) \n",
    "    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
    "    #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
    "\n",
    "    #1. mlm loss\n",
    "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    #2. nsp loss\n",
    "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
    "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
    "    \n",
    "    #3. combine loss\n",
    "    loss = loss_lm + loss_nsp\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040f656",
   "metadata": {},
   "source": [
    "#### Step 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f96a637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '``', 'turn', 'up', 'the', 'music', '``', 'also', 'reached', 'the', 'top', 'twenty', 'in', 'some', 'european', 'countries', 'it', 'peaked', 'at', 'number', '15', 'on', 'the', 'hungarian', 'airplay', 'chart', 'and', 'number', '20', 'on', 'the', 'slovak', 'airplay', 'chart', 'on', 'the', 'irish', 'singles', 'chart', '``', 'turn', 'up', 'the', 'music', '``', 'peaked', 'at', 'number', '12', 'and', 'spent', '18', 'weeks', 'in', 'the', 'chart', 'in', 'the', 'united', 'kingdom', 'the', 'song', 'first', 'charted', 'on', '[MASK]', '10', '2012', 'in', 'a', 'cover', 'version', 'by', 'beautiful', 'people', 'reaching', 'the', 'top', '100', 'on', 'april', '7', '2012', 'the', 'chris', 'brown', 'version', 'having', 'sold', '83', '@', '@', '777', 'copies', 'in', 'its', 'first', 'week', 'became', 'brown', \"'s\", 'first', 'uk', 'number', 'one', 'single', 'that', 'same', 'week', 'it', 'also', 'debuted', 'at', 'number', 'one', 'on', 'the', 'scottish', 'singles', 'chart', '``', 'turn', 'up', 'the', 'music', '``', 'has', 'sold', '273', '@', '@', '000', 'copies', 'in', 'the', 'uk', 'as', 'of', 'december', '2012', '[SEP]', 'people', 'writer', 'chuck', 'arnold', 'wrote', 'that', 'spears', '``', 'never', 'really', 'hit', 'her', 'old', 'stride', '[', ']', '[MASK]', 'was', 'a', 'lot', 'more', 'strutting', 'than', 'real', 'choreographic', 'feats', 'from', '[', 'her', ']', '``', 'jeff', 'montgomery', 'of', 'mtv', 'both', 'praised', 'and', 'dismissed', 'spears', \"'s\", 'performance', 'saying', '``', 'yes', 'welcome', 'to', 'britney', \"'s\", 'circus', 'a', 'big', 'huge', 'loud', 'funny', 'nonsensical', 'three', '@', '@', 'ring', 'affair', 'she', 'looks', 'great', 'in', 'her', 'myriad', 'of', 'outfits', 'and', 'she', 'can', 'still', 'move', 'with', 'the', 'best', 'of', 'them', '[', ']', 'it', \"'s\", 'just', 'well', 'she', \"'s\", 'almost', 'lost', 'in', 'the', 'sheer', 'hugeness', 'of', 'the', 'production', 'around', 'her', '``', 'jane', 'stevenson', 'of', 'toronto', 'sun', 'gave', 'spears', \"'s\", 'performance', 'three', 'out', 'of', 'five', 'stars', 'stating', 'there', 'was', '``', 'so', 'much', 'was', 'going', 'on', '–', 'there', 'were', 'also', 'martial', 'artists', 'bicyclists', 'etc', '–', 'there', 'was', 'no', 'time', 'to', 'really', 'assess', 'spears', 'other', 'than', 'to', 'note', 'that', 'she', 'looked', 'great', '[', ']', 'she', 'could', 'lip', '[MASK]', '@', 'synch', 'the', 'words', '[MASK]', 'one', 'can', 'assume', ')', 'and', 'strut', 'around', 'the', 'stage', 'well', 'enough', 'but', 'there', 'was', 'little', 'in', 'the', 'way', 'of', 'genuine', 'passion', 'joy', 'or', 'excitement', 'on', 'her', 'part', '``', 'the', 'hollywood', 'reporter', \"'s\", 'craig', 'rosen', 'claimed', 'that', '``', 'in', 'the', 'end', 'britney', 'and', 'company', 'delivered', 'an', 'entertaining', 'spectacle', 'but', 'one', 'couldn', \"'t\", 'help', 'but', 'wish', 'that', 'she', 'would', 'strip', 'it', 'all', 'down', 'and', 'show', 'a', 'little', 'more', 'of', 'herself', '``', 'sean', 'daly', 'of', 'st', 'petersburg', 'times', 'summed', 'up', 'all', 'the', 'reviews', 'by', 'stating', '``', 'when', 'britney', 'touring', 'behind', 'her', 'new', 'circus', 'album', 'plays', 'the', 'times', 'forum', 'there', 'will', 'be', 'as', 'many', 'people', 'rooting', 'for', 'her', 'success', 'as', 'her', 'failure', '[', ']', 'but', 'in', 'the', 'end', 'we', \"'re\", 'all', 'envious', 'and', 'thankful', 'jealous', 'mielies', 'applauding', 'we', 'like', 'them', '/', 'us', 'and', 'hate', 'them', '/', 'us', 'for', 'the', 'very', 'same', 'reasons', '``', '[SEP]']\n",
      "masked tokens (words) :  ['and', 'there', 'march', '@', '(']\n",
      "masked tokens list :  [83500, 443148, 215063, 510825, 171827]\n",
      "masked tokens (words) :  ['and', 'and', 'and', 'and', 'and']\n",
      "predict masked tokens list :  [np.int64(83500), np.int64(83500), np.int64(83500), np.int64(83500), np.int64(83500)]\n",
      "1\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
    "input_ids, segment_ids, masked_tokens, masked_pos = input_ids.to(device), segment_ids.to(device), masked_tokens.to(device), masked_pos.to(device)\n",
    "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
    "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
    "\n",
    "#predict masked tokens\n",
    "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.cpu().numpy()\n",
    "#note that zero is padding we add to the masked_tokens\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
    "\n",
    "#predict nsp\n",
    "logits_nsp = logits_nsp.data.max(1)[1][0].data.cpu().numpy()\n",
    "print(logits_nsp)\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_nsp else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd2a09",
   "metadata": {},
   "source": [
    "\n",
    "### 3) Save the trained model weights for later use in Task 2.\n",
    "NOTE: BERT-update.ipynb is available to use CUDA.\n",
    "NOTE: You may refer to the BERT $paper^1$ and use large corpora such as $BookCorpus^2$ or English\n",
    "$Wikipedia^3$. However, you should only use a subset, such as 100k samples, rather than the entire dataset.\n",
    "\n",
    "1 https://aclanthology.org/N19-1423.pdf\n",
    "\n",
    "2 https://huggingface.co/datasets/bookcorpus/bookcorpus\n",
    "\n",
    "3 https://huggingface.co/datasets/legacy-datasets/wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e5b7c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model checkpoint saved!\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoints\n",
    "_MODEL_CHECKPOINT_PATH = \"../models\"\n",
    "_MODEL_CHECKPOINT_FILENAME = \"bert_checkpoint.pt\"\n",
    "checkpoint_full_path = _MODEL_CHECKPOINT_PATH+ \"/\" + _MODEL_CHECKPOINT_FILENAME\n",
    "\n",
    "torch.save([model.params, model.state_dict()], checkpoint_full_path)\n",
    "print(\"BERT Model checkpoint saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e17068ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "params, state_dict = torch.load(f'{checkpoint_full_path}', map_location=device)\n",
    "\n",
    "# Recreate model with saved hyperparameters\n",
    "loaded_model = BERT(**params, device=device).to(device)\n",
    "\n",
    "# Load weights\n",
    "loaded_model.load_state_dict(state_dict)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded from checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a05a74",
   "metadata": {},
   "source": [
    "## Task 2. Sentence Embedding with Sentence BERT - Implement trained BERT from task 1 with\n",
    "siamese network structures to derive semantically meaningful sentence embeddings that can be compared\n",
    "using cosine-similarity. (3 points)\n",
    "1) Use the $SNLI^4$ OR $MNLI^5$ datasets from Hugging Face, or any dataset related to classification\n",
    "tasks.\n",
    "2) Reproduce training the Sentence-BERT as described in the $paper^6$.\n",
    "3) Focus on the Classification Objective Function: (SoftmaxLoss)\n",
    "$$o = softmax (W^T·(u, v, |u −v|))$$\n",
    "HINT : You can take a look how to implement Softmax loss in the file 04 - Huggingface/Appendix -\n",
    "Sentence Embedding/S-BERT.ipynb.\n",
    "1https://aclanthology.org/N19-1423.pdf2https://huggingface.co/datasets/bookcorpus/bookcorpus3https://huggingface.co/datasets/legacy-datasets/wikipedia4https://huggingface.co/datasets/snli5https://huggingface.co/datasets/glue/viewer/mnli6https://aclanthology.org/D19-1410/\n",
    "1\n",
    "2\n",
    "\n",
    "4 https://huggingface.co/datasets/stanfordnlp/snli\n",
    "\n",
    "5 https://huggingface.co/datasets/glue/viewer/mnli\n",
    "\n",
    "6 https://aclanthology.org/D19-1410/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498b954",
   "metadata": {},
   "source": [
    "## Task 3. Evaluation and Analysis (1 points)\n",
    "1) Provide the performance metrics (classification Report) based on the SNLI or MNLI datasets for\n",
    "the Natural Language Inference (NLI) task.\n",
    "precision recall f1-score support\n",
    "entailment 0.42 0.02 0.05 3486\n",
    "neutral 0.33 0.75 0.46 3199\n",
    "contradiction 0.33 0.25 0.28 3315\n",
    "accuracy 0.33 10000\n",
    "macro avg 0.36 0.34 0.26 10000\n",
    "weighted avg 0.36 0.33 0.26 10000\n",
    "Table 1. Sample of Classification Report\n",
    "2) Discuss any limitations or challenges encountered during the implementation and propose potential\n",
    "improvements or modifications.\n",
    "NOTE: Make sure to provide proper documentation, including details of the datasets used, hyperpa-\n",
    "rameters, and any modifications made to the original models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2714a92",
   "metadata": {},
   "source": [
    "## Task 4. Text similarity - Web Application Development - Develop a simple web application that\n",
    "demonstrates the capabilities of your text-embedding model. (1 points)\n",
    "1) Develop a simple website with two input boxes for search queries.\n",
    "2) Utilize a custom-trained sentence transformer model to predict Natural Language Inference (NLI)\n",
    "Task (entailment, neutral and contradiction).\n",
    "For example:\n",
    "• Premise: A man is playing a guitar on stage.\n",
    "• Hypothesis: The man is performing music.\n",
    "• Label: Entailment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
