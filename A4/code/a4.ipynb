{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3c9b74",
   "metadata": {},
   "source": [
    "# A4: Do you AGREE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551a234",
   "metadata": {},
   "source": [
    "This assignment will guide you in training a pre-trained model like BERT from scratch, focusing onleveraging text embeddings to capture semantic similarity. \n",
    "\n",
    "Additionally, we will explore how to adapt the loss function for tasks like Natural Language Inference (NLI) to enhance the model’s ability to understand semantic relationships between texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45d9ef",
   "metadata": {},
   "source": [
    "#### Step 0: Prepare Environment - Import Libraries and select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e37322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import datasets, math, re, random, time\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7389429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mimimum required torch version for MPS support \"1.12+\"\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8931caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub login token\n",
    "# Make sure to set the HF_TOKEN environment variable in your .env file with your Hugging Face token\n",
    "import os\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde7b574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# universal device selection: use gpu if available, else cpu\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear CUDA cache to free up memory \n",
    "        return torch.device(\"cuda\")      # NVIDIA GPU\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()  # Clear MPS cache to avoid memory issues\n",
    "        return torch.device(\"mps\")       # Apple Silicon GPU\n",
    "    else:\n",
    "        torch.empty_cache()  # Clear CPU cache to free up memory\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c47e7",
   "metadata": {},
   "source": [
    "## Task 1. Training BERT from Scratch - Based on Masked Language Model/BERT-update.ipynb, modify as follows: (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b45515",
   "metadata": {},
   "source": [
    "### 1.1) Implement Bidirectional Encoder Representations from Transformers (BERT) from scratch, following the concepts learned in class.\n",
    "\n",
    "NOTE: BERT-update.ipynb is available to use CUDA.\n",
    "NOTE: You may refer to the BERT $paper^1$ and use large corpora such as $BookCorpus^2$ or English\n",
    "$Wikipedia^3$. However, you should only use a subset, such as 100k samples, rather than the entire dataset.\n",
    "\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca263a1c",
   "metadata": {},
   "source": [
    "#### Step 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70f9a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub login token\n",
    "# Make sure to set the HF_TOKEN environment variable in your .env file with your Hugging Face token\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f59994",
   "metadata": {},
   "source": [
    "Using dataset from [Salesforce wikitext](https://huggingface.co/datasets/Salesforce/wikitext)\n",
    "\n",
    "Info about Dataset:\n",
    "wikitext-103-raw-v1\n",
    "```sh\n",
    "    Size of downloaded dataset files: 191.98 MB\n",
    "    Size of the generated dataset: 549.42 MB\n",
    "    Total amount of disk used: 741.41 MB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1f76aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1801350\n",
      "Validation set size: 3760\n",
      "Test set size: 4358\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# data folder are not uploaded to Github.\n",
    "_DATA_PATH = \"../data/wikitext-103\"\n",
    "_DATA_FILENAME = os.path.join(_DATA_PATH, \"wikitext-103-train.arrow\")\n",
    "os.makedirs(_DATA_PATH, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(_DATA_FILENAME):\n",
    "    # Download and save to local folder\n",
    "    dataset_train = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\", cache_dir=_DATA_PATH)\n",
    "    dataset_valid = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\", cache_dir=_DATA_PATH)\n",
    "    dataset_test = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\", cache_dir=_DATA_PATH)\n",
    "\n",
    "else:\n",
    "    # Load from local Parquet file\n",
    "    from datasets import Dataset\n",
    "    dataset_train = Dataset.from_parquet(_DATA_FILENAME)\n",
    "    dataset_valid = Dataset.from_parquet(os.path.join(_DATA_PATH, \"wikitext-103-validation.arrow\"))\n",
    "    dataset_test = Dataset.from_parquet(os.path.join(_DATA_PATH, \"wikitext-103-test.arrow\"))\n",
    "    print(\"Loaded datasets from local Parquet files.\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset_train)}\")\n",
    "print(f\"Validation set size: {len(dataset_valid)}\")\n",
    "print(f\"Test set size: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0350b",
   "metadata": {},
   "source": [
    "Check dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76001bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48eef587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4358\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a55249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3760\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9221afc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Va',\n",
       " ' The game began development in 2010 , carrying over a large portion of the work ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 entries in the dataset \n",
    "# with only the first 80 characters of the text for brevity\n",
    "[text[:80] for text in dataset_train[:5]['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db211b",
   "metadata": {},
   "source": [
    "#### Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca1ab4",
   "metadata": {},
   "source": [
    "Before making the vocabs, remove special characters and transform text into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f984fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [re.sub(\"[.,!?\\\\-]\", '', t.lower()) for t in dataset_train[\"text\"] if t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8ed77fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' = valkyria chronicles iii = \\n',\n",
       " ' senjō no valkyria 3 : unrecorded chronicles ( japanese : 戦場のヴァルキュリア3  lit  valkyria of the battlefield 3 )  commonly referred to as valkyria chronicles iii outside japan  is a tactical role @@ playing video game developed by sega and mediavision for the playstation portable  released in january 2011 in japan  it is the third game in the valkyria series  employing the same fusion of tactical and real @@ time gameplay as its predecessors  the story runs parallel to the first game and follows the \" nameless \"  a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \"  \\n',\n",
       " \" the game began development in 2010  carrying over a large portion of the work done on valkyria chronicles ii  while it retained the standard features of the series  it also underwent multiple adjustments  such as making the game more forgiving for series newcomers  character designer raita honjou and composer hitoshi sakimoto both returned from previous entries  along with valkyria chronicles ii director takeshi ozawa  a large team of writers handled the script  the game 's opening theme was sung by may 'n  \\n\",\n",
       " \" it met with positive sales in japan  and was praised by both japanese and western critics  after release  it received downloadable content  along with an expanded edition in november of that year  it was also adapted into manga and an original video animation series  due to low sales of valkyria chronicles ii  valkyria chronicles iii was not localized  but a fan translation compatible with the game 's expanded edition was released in 2014  mediavision would return to the franchise with the development of valkyria : azure revolution for the playstation 4  \\n\",\n",
       " ' = = gameplay = = \\n']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d795b64",
   "metadata": {},
   "source": [
    "#### Step 3: Build Vocabulary - Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1f6fd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190fc3868c1d473cb493a6df0f676ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating word2id: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "539719"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Combine everything into one to make vocab\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  # special tokens\n",
    "\n",
    "# Create the word2id in a single pass\n",
    "for i, w in tqdm(enumerate(word_list), desc=\"Creating word2id\"):\n",
    "    word2id[w] = i + 4  # because 0-3 are already occupied\n",
    "\n",
    "# Precompute the id2word mapping (this can be done once after word2id is fully populated)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "798d8ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2word[0] = [PAD]\n",
      "id2word[1] = [CLS]\n",
      "id2word[2] = [SEP]\n",
      "id2word[3] = [MASK]\n",
      "id2word[4] = ἁνθρώπου\n",
      "id2word[5] = kirsopp\n",
      "id2word[6] = soarin\n",
      "id2word[7] = ruckdeschel\n",
      "id2word[8] = gubarev\n",
      "id2word[9] = vagharshapat\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"id2word[{i}] = {id2word[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1982a",
   "metadata": {},
   "source": [
    "#### Step 3: Tokenization \n",
    "\n",
    "using spaCy tokenizer or NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ccc20ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e7d09fce4e4541bb5ec577fcb859e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing sentences:   0%|          | 0/1165029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of all tokens for the whole text\n",
    "token_list = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "for sentence in tqdm(sentences, desc=\"Processing sentences\"):\n",
    "    token_list.append([word2id[word] for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40ef2137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences[0] => ['=', 'valkyria', 'chronicles', 'iii', '=']...\n",
      "token_list[0] => [177347, 259061, 100513, 372364, 177347]...\n",
      "sentences[1] => ['senjō', 'no', 'valkyria', '3', ':', 'unrecorded', 'chronicles', '(', 'japanese', ':']...\n",
      "token_list[1] => [246676, 488586, 259061, 446802, 30650, 190007, 100513, 230477, 375006, 30650]...\n",
      "sentences[2] => ['the', 'game', 'began', 'development', 'in', '2010', 'carrying', 'over', 'a', 'large']...\n",
      "token_list[2] => [65286, 78660, 345996, 287783, 335490, 244805, 407664, 350545, 272677, 399248]...\n",
      "sentences[3] => ['it', 'met', 'with', 'positive', 'sales', 'in', 'japan', 'and', 'was', 'praised']...\n",
      "token_list[3] => [342902, 197976, 167790, 335913, 417748, 335490, 394738, 85077, 42664, 55868]...\n",
      "sentences[4] => ['=', '=', 'gameplay', '=', '=']...\n",
      "token_list[4] => [177347, 177347, 148161, 177347, 177347]...\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"sentences[{i}] => {sentences[i].split()[:10]}...\")\n",
    "    print(f\"token_list[{i}] => {token_list[i][:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd9be5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = Valkyria Chronicles III = \\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0d44a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in raw text: 540095682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  = Valkyria Chronicles III = \\n   Senjō no Valkyria 3 : Unrecorded Chronicles ( '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = \" \".join(dataset_train['text']) # all sentences in a line\n",
    "print(f\"Total characters in raw text: {len(raw_text)}\")\n",
    "raw_text[:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf4cde",
   "metadata": {},
   "source": [
    "Simple space based tokenization used here. spaCy and NLTK are dropped for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e5ed9",
   "metadata": {},
   "source": [
    "Too big dataset. It will fail spacy max_lenght limit validation. The validation exists to prevent memory allocation error. Don't concatenate, but use for loop on whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f62bc",
   "metadata": {},
   "source": [
    "Since spaCy is slow, Using NLTK.\n",
    "\n",
    "The command nltk.download('punkt') downloads the \"punkt\" tokenizer models for NLTK. \"Punkt\" is a pre-trained model used by NLTK for sentence splitting and word tokenization in English and other languages. Without downloading \"punkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03f94e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used NLTK for tokenization, simple spacebased tokenization is used instead. \n",
    "# not necessary for this assignment.\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# _DOWNLOAD_DIR = \"../models/nltk_data\"\n",
    "# os.makedirs(_DOWNLOAD_DIR, exist_ok=True)\n",
    "# nltk.download('punkt', download_dir=_DOWNLOAD_DIR)\n",
    "# nltk.download('punkt_tab', download_dir=_DOWNLOAD_DIR)\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# nltk.data.path.append(_DOWNLOAD_DIR)\n",
    "# tokenized_texts = [word_tokenize(text) for text in sentences]\n",
    "\n",
    "# # Example: print the first 5 tokenized samples\n",
    "# for tokens in tokenized_texts[:5]:\n",
    "    # print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64c05f",
   "metadata": {},
   "source": [
    "\n",
    "### 2) Train the model on a suitable dataset. Ensure to source this dataset from reputable public databases or repositories. It is imperative to give proper credit to the dataset source in your documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178b3bc",
   "metadata": {},
   "source": [
    "#### Step 5: Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb7e27",
   "metadata": {},
   "source": [
    "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
    "\n",
    "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
    "\n",
    "2. **Segment embedding**\n",
    "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
    "\n",
    "3. **Masking**\n",
    "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
    "\n",
    "4. **Padding**\n",
    "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
    "\n",
    "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c9fda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5 #even though it does not reach 15% yet....maybe you can set this threshold\n",
    "max_len    = 1000 #maximum length that my transformer will accept.....all sentence will be padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b26a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    half_batch_size = batch_size // 2\n",
    "    positive = negative = 0\n",
    "    while positive != half_batch_size or negative != half_batch_size:\n",
    "\n",
    "        #randomly choose two sentence\n",
    "        tokens_a_index, tokens_b_index = np.random.randint(len(sentences), size=2)\n",
    "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        #1. token embedding - add CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "        #2. segment embedding - which sentence is 0 and 1\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]']\n",
    "                                 and token != word2id['[SEP]']]\n",
    "        np.random.shuffle(candidates_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            rand_val = np.random.random()\n",
    "            if rand_val < 0.1:  #10% replace with random token\n",
    "                index = np.random.randint(4, vocab_size - 1)  # random token should not involve [PAD], [CLS], [SEP], [MASK]\n",
    "                input_ids[pos] = word2id[id2word[index]]\n",
    "            elif rand_val < 0.8:  #80 replace with [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        #6. check whether is positive or negative\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < half_batch_size:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < half_batch_size:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d20c9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6c5696cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77bc9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cd4e809a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1000]),\n",
       " torch.Size([6, 1000]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6, 5]),\n",
       " tensor([0, 0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "186acf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 65286, 119198,  57329, 139900, 285794],\n",
       "        [146375,  66567,  33021,      0,      0],\n",
       "        [ 63192, 531369, 192655, 108065,  65286],\n",
       "        [532999, 421534, 532999,  12867,  45351],\n",
       "        [ 69515, 178483, 299341,      0,      0],\n",
       "        [485383,  65286,  22983,   2190, 497091]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79222c25",
   "metadata": {},
   "source": [
    "#### Step 6. Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52993252",
   "metadata": {},
   "source": [
    "\n",
    "Recall that BERT only uses the encoder.\n",
    "\n",
    "BERT has the following components:\n",
    "\n",
    "- Embedding layers\n",
    "- Attention Mask\n",
    "- Encoder layer\n",
    "- Multi-head attention\n",
    "- Scaled dot product attention\n",
    "- Position-wise feed-forward network\n",
    "- BERT (assembling all the components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06ca61",
   "metadata": {},
   "source": [
    "##### 6.1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae014de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.device = device\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2460af",
   "metadata": {},
   "source": [
    "##### 6.2 Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, device=None):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceedd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd77f3",
   "metadata": {},
   "source": [
    "##### 6.3 Encoder\n",
    "\n",
    "The encoder has two main components: \n",
    "\n",
    "- Multi-head Attention\n",
    "- Position-wise feed-forward network\n",
    "\n",
    "First let's make the wrapper called `EncoderLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k)\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46167a91",
   "metadata": {},
   "source": [
    "Let's define the scaled dot attention, to be used inside the multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a453c5",
   "metadata": {},
   "source": [
    "Let's define the parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7c98a",
   "metadata": {},
   "source": [
    "Here is the Multiheadattention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k  # d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_k, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "\n",
    "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
    "        output = self.linear(context)\n",
    "        return self.norm(output + residual), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8194d8",
   "metadata": {},
   "source": [
    "Here is the PoswiseFeedForwardNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe590f9a",
   "metadata": {},
   "source": [
    "##### 6.4 Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a4378",
   "metadata": {},
   "source": [
    "#### Step 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff20801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epoch = 500\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ").to(device) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "pbar = tqdm(range(num_epoch), desc=\"Training BERT\")\n",
    "for epoch in pbar:\n",
    "    batch = make_batch(token_list)  # fresh batch each epoch\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    input_ids = input_ids.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    masked_tokens = masked_tokens.to(device)\n",
    "    masked_pos = masked_pos.to(device)\n",
    "    isNext = isNext.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos) \n",
    "\n",
    "    #1. mlm loss\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    #2. nsp loss\n",
    "    loss_nsp = criterion(logits_nsp, isNext)\n",
    "    \n",
    "    #3. combine loss\n",
    "    loss = loss_lm + loss_nsp\n",
    "    if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epoch} - Loss: {loss.item():.4f} (MLM: {loss_lm.item():.4f}, NSP: {loss_nsp.item():.4f})\")\n",
    "    pbar.set_postfix(loss=f\"{loss:.4f}\", mlm=f\"{loss_lm:.4f}\", nsp=f\"{loss_nsp:.4f}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'\\nTraining complete! Final loss = {loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040f656",
   "metadata": {},
   "source": [
    "#### Step 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
    "input_ids, segment_ids, masked_tokens, masked_pos = input_ids.to(device), segment_ids.to(device), masked_tokens.to(device), masked_pos.to(device)\n",
    "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
    "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
    "\n",
    "#predict masked tokens\n",
    "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.cpu().numpy()\n",
    "#note that zero is padding we add to the masked_tokens\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
    "\n",
    "#predict nsp\n",
    "logits_nsp = logits_nsp.data.max(1)[1][0].data.cpu().numpy()\n",
    "print(logits_nsp)\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_nsp else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cb741",
   "metadata": {},
   "source": [
    "#### Step 8: Model Evaluation\n",
    "\n",
    "Evaluate the BERT model on multiple batches to measure:\n",
    "- **MLM Accuracy**: How well the model predicts the masked tokens (excluding padding)\n",
    "- **NSP Accuracy**: How well the model predicts whether sentence B follows sentence A\n",
    "- **Combined Loss**: Average loss across evaluation batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_valid_text = dataset_valid[\"text\"]\n",
    "\n",
    "tokenized_texts_valid = [word_tokenize(text) for text in sentences_valid_text]\n",
    "\n",
    "word_list_valid = list(set(\" \".join([sent for sublist in tokenized_texts_valid for sent in sublist]).split()))\n",
    "\n",
    "\n",
    "# Example: print the first 5 tokenized samples\n",
    "for tokens in tokenized_texts_valid[:5]:\n",
    "    print(tokens)\n",
    "\n",
    "print(f\"Number of tokenized validation samples: {len(tokenized_texts_valid)}\")\n",
    "\n",
    "\n",
    "# Word to ID conversion for validation set\n",
    "token_list_valid, _ , _ = word_to_token_id(word_list_valid)\n",
    "\n",
    "# Example: print the first 5 tokenized samples\n",
    "for token in token_list_valid[:5]:\n",
    "    print(token[:10])  # Print first 10 token IDs for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95458999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "n_eval_batches = 50\n",
    "total_mlm_correct = 0\n",
    "total_mlm_total = 0\n",
    "total_nsp_correct = 0\n",
    "total_nsp_total = 0\n",
    "total_loss = 0.0\n",
    "\n",
    "# Pre-generate all batches on CPU first (the bottleneck is make_batch(), not GPU)\n",
    "print(f\"Pre-generating {n_eval_batches} evaluation batches...\")\n",
    "eval_batches = []\n",
    "for _ in tqdm(range(n_eval_batches), desc=\"Generating batches\"):\n",
    "    b = make_batch(token_list_valid)\n",
    "    ids, segs, mtoks, mpos, nxt = map(torch.LongTensor, zip(*b))\n",
    "    eval_batches.append((ids, segs, mtoks, mpos, nxt))\n",
    "\n",
    "# Run inference on GPU in a tight loop\n",
    "print(f\"Running evaluation on {device}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ids, segs, mtoks, mpos, nxt in tqdm(eval_batches, desc=\"Evaluating\"):\n",
    "        # Move to device with non_blocking for async transfer\n",
    "        ids   = ids.to(device, non_blocking=True)\n",
    "        segs  = segs.to(device, non_blocking=True)\n",
    "        mtoks = mtoks.to(device, non_blocking=True)\n",
    "        mpos  = mpos.to(device, non_blocking=True)\n",
    "        nxt   = nxt.to(device, non_blocking=True)\n",
    "\n",
    "        logits_lm, logits_nsp = model(ids, segs, mpos)\n",
    "\n",
    "        # --- Loss ---\n",
    "        loss_lm = criterion(logits_lm.transpose(1, 2), mtoks).float().mean()\n",
    "        loss_nsp = criterion(logits_nsp, nxt)\n",
    "        total_loss += (loss_lm + loss_nsp).item()\n",
    "\n",
    "        # --- MLM Accuracy (ignore padding tokens where masked_tokens == 0) ---\n",
    "        pred_lm = logits_lm.argmax(dim=2)  # faster than .data.max(2)[1]\n",
    "        non_pad_mask = mtoks != 0\n",
    "        total_mlm_correct += (pred_lm[non_pad_mask] == mtoks[non_pad_mask]).sum().item()\n",
    "        total_mlm_total += non_pad_mask.sum().item()\n",
    "\n",
    "        # --- NSP Accuracy ---\n",
    "        pred_nsp = logits_nsp.argmax(dim=1)  # faster than .data.max(1)[1]\n",
    "        total_nsp_correct += (pred_nsp == nxt).sum().item()\n",
    "        total_nsp_total += nxt.size(0)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "avg_loss = total_loss / n_eval_batches\n",
    "mlm_accuracy = total_mlm_correct / total_mlm_total * 100 if total_mlm_total > 0 else 0\n",
    "nsp_accuracy = total_nsp_correct / total_nsp_total * 100 if total_nsp_total > 0 else 0\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Evaluation Results over {n_eval_batches} batches ({n_eval_batches * batch_size} samples)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Average Loss     : {avg_loss:.4f}\")\n",
    "print(f\"  MLM Accuracy     : {mlm_accuracy:.2f}% ({total_mlm_correct}/{total_mlm_total})\")\n",
    "print(f\"  NSP Accuracy     : {nsp_accuracy:.2f}% ({total_nsp_correct}/{total_nsp_total})\")\n",
    "print(f\"  Inference Time   : {elapsed:.2f}s ({elapsed/n_eval_batches*1000:.1f}ms per batch)\")\n",
    "print(f\"  Device           : {device}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Switch back to training mode\n",
    "model.train()\n",
    "print(\"\\nModel set back to training mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd2a09",
   "metadata": {},
   "source": [
    "\n",
    "### 3) Save the trained model weights for later use in Task 2.\n",
    "\n",
    "1 https://aclanthology.org/N19-1423.pdf\n",
    "\n",
    "2 https://huggingface.co/datasets/bookcorpus/bookcorpus\n",
    "\n",
    "3 https://huggingface.co/datasets/legacy-datasets/wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoints\n",
    "_MODEL_CHECKPOINT_PATH = \"../models\"\n",
    "_MODEL_CHECKPOINT_FILENAME = \"bert_checkpoint.pt\"\n",
    "checkpoint_full_path = _MODEL_CHECKPOINT_PATH+ \"/\" + _MODEL_CHECKPOINT_FILENAME\n",
    "\n",
    "torch.save([model.params, model.state_dict()], checkpoint_full_path)\n",
    "print(\"BERT Model checkpoint saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a05a74",
   "metadata": {},
   "source": [
    "## Task 2. Sentence Embedding with Sentence BERT - Implement trained BERT from task 1 with siamese network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. (3 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a976bc",
   "metadata": {},
   "source": [
    "1) Use the $SNLI^4$ OR $MNLI^5$ datasets from Hugging Face, or any dataset related to classification\n",
    "tasks.\n",
    "2) Reproduce training the Sentence-BERT as described in the $paper^6$.\n",
    "3) Focus on the Classification Objective Function: (SoftmaxLoss)\n",
    "$$o = softmax (W^T·(u, v, |u −v|))$$\n",
    "HINT : You can take a look how to implement Softmax loss in the file 04 - Huggingface/Appendix -\n",
    "Sentence Embedding/S-BERT.ipynb.\n",
    "\n",
    "<img src=\"../images/sbert-architecture.png\">\n",
    "\n",
    "\n",
    "4 https://huggingface.co/datasets/stanfordnlp/snli\n",
    "\n",
    "5 https://huggingface.co/datasets/glue/viewer/mnli\n",
    "\n",
    "6 https://aclanthology.org/D19-1410/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274b87e",
   "metadata": {},
   "source": [
    "#### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "snli = datasets.load_dataset('snli')\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli['train'].features, snli['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf825aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd01018",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d892d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "snli = snli.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8886e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3404697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your two DatasetDict objects named snli and mnli\n",
    "from datasets import DatasetDict\n",
    "# Merge the two DatasetDict objects\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': datasets.concatenate_datasets([snli['train'], mnli['train']]).shuffle(seed=55).select(list(range(1000))),\n",
    "    'test': datasets.concatenate_datasets([snli['test'], mnli['test_mismatched']]).shuffle(seed=55).select(list(range(100))),\n",
    "    'validation': datasets.concatenate_datasets([snli['validation'], mnli['validation_mismatched']]).shuffle(seed=55).select(list(range(1000)))\n",
    "})\n",
    "#remove .select(list(range(1000))) in order to use full dataset\n",
    "# Now, merged_dataset_dict contains the combined datasets from snli and mnli\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76058f4b",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55e3e7",
   "metadata": {},
   "source": [
    "Using CustomBertTokenizer built in Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owwk6sph4lb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tokenizer using Task 1 vocabulary\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class CustomBERTTokenizer:\n",
    "    \"\"\"Custom tokenizer that uses the word2id from Task 1\"\"\"\n",
    "    \n",
    "    def __init__(self, word2id, max_length=128):\n",
    "        self.word2id = word2id\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = word2id[_PAD_TOKEN]\n",
    "        self.cls_token_id = word2id[_CLS_TOKEN]\n",
    "        self.sep_token_id = word2id[_SEP_TOKEN]\n",
    "        self.unk_token_id = word2id.get('[UNK]', 0)  # Use [PAD] as fallback for unknown\n",
    "        \n",
    "    def __call__(self, text, return_tensors=None, max_length=None, truncation=True, padding=True):\n",
    "        \"\"\"Tokenize text using NLTK and Task 1 vocabulary\"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "            \n",
    "        # Preprocess: lowercase and remove punctuation (same as Task 1)\n",
    "        text = re.sub(\"[.,!?\\\\-]\", '', text.lower())\n",
    "        \n",
    "        # Tokenize using NLTK (same as Task 1)\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Convert to IDs, using [PAD] for unknown words\n",
    "        token_ids = [self.word2id.get(token, self.pad_token_id) for token in tokens]\n",
    "        \n",
    "        # Add [CLS] at start and [SEP] at end\n",
    "        token_ids = [self.cls_token_id] + token_ids + [self.sep_token_id]\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        \n",
    "        # Truncate if needed\n",
    "        if truncation and len(token_ids) > max_length:\n",
    "            token_ids = token_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "            # Ensure [SEP] at end after truncation\n",
    "            token_ids[-1] = self.sep_token_id\n",
    "        \n",
    "        # Pad if needed\n",
    "        if padding:\n",
    "            padding_length = max_length - len(token_ids)\n",
    "            if padding_length > 0:\n",
    "                token_ids = token_ids + [self.pad_token_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "        \n",
    "        # Return in HuggingFace format\n",
    "        result = {\n",
    "            'input_ids': token_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        \n",
    "        # Convert to tensors if requested\n",
    "        if return_tensors == 'pt':\n",
    "            import torch\n",
    "            result['input_ids'] = torch.tensor([result['input_ids']])\n",
    "            result['attention_mask'] = torch.tensor([result['attention_mask']])\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Create the custom tokenizer\n",
    "custom_tokenizer = CustomBERTTokenizer(word2id, max_length=128)\n",
    "\n",
    "# Test it\n",
    "test_sentence = \"The cat is walking on the street.\"\n",
    "test_output = custom_tokenizer(test_sentence, return_tensors='pt')\n",
    "print(f\"Test sentence: {test_sentence}\")\n",
    "print(f\"Input IDs shape: {test_output['input_ids'].shape}\")\n",
    "print(f\"First 10 token IDs: {test_output['input_ids'][0, :10]}\")\n",
    "print(f\"Attention mask shape: {test_output['attention_mask'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda07a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess using custom tokenizer from Task 1\"\"\"\n",
    "    max_seq_length = 128\n",
    "    \n",
    "    # Process each example individually since custom tokenizer doesn't support batching\n",
    "    premise_input_ids = []\n",
    "    premise_attention_mask = []\n",
    "    hypothesis_input_ids = []\n",
    "    hypothesis_attention_mask = []\n",
    "    \n",
    "    # Get the number of examples\n",
    "    num_examples = len(examples['premise'])\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Tokenize premise\n",
    "        premise_result = custom_tokenizer(\n",
    "            examples['premise'][i], \n",
    "            max_length=max_seq_length, \n",
    "            truncation=True, \n",
    "            padding=True\n",
    "        )\n",
    "        premise_input_ids.append(premise_result['input_ids'])\n",
    "        premise_attention_mask.append(premise_result['attention_mask'])\n",
    "        \n",
    "        # Tokenize hypothesis\n",
    "        hypothesis_result = custom_tokenizer(\n",
    "            examples['hypothesis'][i], \n",
    "            max_length=max_seq_length, \n",
    "            truncation=True, \n",
    "            padding=True\n",
    "        )\n",
    "        hypothesis_input_ids.append(hypothesis_result['input_ids'])\n",
    "        hypothesis_attention_mask.append(hypothesis_result['attention_mask'])\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = examples[\"label\"]\n",
    "    \n",
    "    return {\n",
    "        \"premise_input_ids\": premise_input_ids,\n",
    "        \"premise_attention_mask\": premise_attention_mask,\n",
    "        \"hypothesis_input_ids\": hypothesis_input_ids,\n",
    "        \"hypothesis_attention_mask\": hypothesis_attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "print(\"Preprocessing with custom tokenizer...\")\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing with custom vocabulary\"\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise', 'hypothesis', 'label'])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(f\"Dataset preprocessed with custom tokenizer!\")\n",
    "print(f\"Train size: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Validation size: {len(tokenized_datasets['validation'])}\")\n",
    "print(f\"Test size: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681b85c",
   "metadata": {},
   "source": [
    "#### 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a36b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['premise_input_ids'].shape)\n",
    "    print(batch['premise_attention_mask'].shape)\n",
    "    print(batch['hypothesis_input_ids'].shape)\n",
    "    print(batch['hypothesis_attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c4438",
   "metadata": {},
   "source": [
    "#### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17068ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "params, state_dict = torch.load(f'{checkpoint_full_path}', map_location=device)\n",
    "\n",
    "# Recreate model with saved hyperparameters\n",
    "loaded_model = BERT(**params, device=device).to(device)\n",
    "\n",
    "# Load weights\n",
    "loaded_model.load_state_dict(state_dict)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded from checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7256192",
   "metadata": {},
   "source": [
    "#### 5. Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d09e4",
   "metadata": {},
   "source": [
    "#### 6. Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7034e",
   "metadata": {},
   "source": [
    "\n",
    "#### Classification Objective Function \n",
    "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t ∈  \\mathbb{R}^{3n \\times k}  $:\n",
    "\n",
    "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
    "\n",
    "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
    "\n",
    "#### Regression Objective Function. \n",
    "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
    "\n",
    "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)\n",
    "\n",
    "<img src=\"../images/sbert-architecture.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u,v):\n",
    "    # build the |u-v| tensor\n",
    "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "    \n",
    "    # concatenate u, v, |u-v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2510c",
   "metadata": {},
   "source": [
    "<img src=\"../images/sbert-ablation.png\" width=\"350\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ca0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
    "\n",
    "# Use LOWER learning rate for BERT encoder (it's already pretrained)\n",
    "# Use HIGHER learning rate for classifier head (training from scratch)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Reduced from 2e-5\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=1e-3)  # Higher for new layers\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Optimizer LR - BERT encoder: 1e-5, Classifier head: 1e-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db96215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(raw_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dd4f1",
   "metadata": {},
   "source": [
    "#### 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2954de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epoch = 5\n",
    "model = loaded_model  # Use the loaded model from Task 1 checkpoint\n",
    "\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()  \n",
    "    classifier_head.train()\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "\n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        segment_ids_a = torch.zeros_like(inputs_ids_a).to(device)\n",
    "        segment_ids_b = torch.zeros_like(inputs_ids_b).to(device)\n",
    "\n",
    "        \n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u_last_hidden_state = model.get_last_hidden_state(inputs_ids_a, segment_ids_a)  # batch_size, seq_len, hidden_dim\n",
    "        v_last_hidden_state = model.get_last_hidden_state(inputs_ids_b, segment_ids_b)  # batch_size, seq_len, hidden_dim\n",
    "\n",
    "         # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
    "        \n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "        \n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "        \n",
    "        # process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x) #batch_size, classifer\n",
    "        \n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, label)\n",
    "        \n",
    "        # # using loss, calculate gradients and then optimizerize\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer_classifier.step()\n",
    "\n",
    "        # scheduler.step() # update learning rate scheduler\n",
    "        # scheduler_classifier.step()\n",
    "\n",
    "        # correct order of scheduler calls\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Optional: gradient clipping for stability\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "        scheduler.step()  # After optimizer!\n",
    "        scheduler_classifier.step()\n",
    "        \n",
    "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "classes = [\"entailment\", \"neutral\", \"contradiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "classifier_head.eval()\n",
    "total_similarity = 0\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # Move batches to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        segment_ids = torch.zeros(inputs_ids_a.shape[0], inputs_ids_a.shape[1], dtype=torch.int32).to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "\n",
    "        # Extract token embeddings from BERT\n",
    "        u = model.get_last_hidden_state(inputs_ids_a, segment_ids)  # (batch_size, seq_len, hidden_dim)\n",
    "        v = model.get_last_hidden_state(inputs_ids_b, segment_ids)  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Get the mean pooled vectors (Keep them as Tensors)\n",
    "        u_mean_pool = mean_pool(u, attention_a)  # (batch_size, hidden_dim)\n",
    "        v_mean_pool = mean_pool(v, attention_b)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Computing cosine similarity\n",
    "        similarity_score = cosine_similarity(u_mean_pool.cpu().numpy().reshape(-1), v_mean_pool.cpu().numpy().reshape(-1))\n",
    "        total_similarity += similarity_score\n",
    "\n",
    "        # Concatenate [u, v, |u - v|]\n",
    "        uv_abs = torch.abs(u_mean_pool - v_mean_pool)  # [batch_size, hidden_dim]\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1)  # [batch_size, 3*hidden_dim]\n",
    "\n",
    "        # Classification\n",
    "        logit_fn = classifier_head(x)  # (batch_size, num_classes)\n",
    "        probs = torch.nn.functional.softmax(logit_fn, dim=-1)\n",
    "\n",
    "        preds = torch.argmax(logit_fn, dim=-1)\n",
    "\n",
    "        labels.extend(label.cpu().tolist())\n",
    "        probabilities.extend(probs.cpu().tolist())\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "average_similarity = total_similarity / len(eval_dataloader)\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4563f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced281ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model checkpoints\n",
    "torch.save([model.params, model.state_dict()], '../models/sentence_bert_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db800130",
   "metadata": {},
   "source": [
    "#### 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs_a = tokenizer(sentence_a, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
    "    inputs_b = tokenizer(sentence_b, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
    "\n",
    "    # Move to device\n",
    "    inputs_ids_a = inputs_a['input_ids'].to(device)\n",
    "    attention_a = inputs_a['attention_mask'].to(device)\n",
    "    inputs_ids_b = inputs_b['input_ids'].to(device)\n",
    "    attention_b = inputs_b['attention_mask'].to(device)\n",
    "\n",
    "    # Create segment_ids (all 0s for single sentences)\n",
    "    segment_ids_a = torch.zeros_like(inputs_ids_a).to(device)\n",
    "    segment_ids_b = torch.zeros_like(inputs_ids_b).to(device)\n",
    "\n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        # Extract embeddings using custom BERT API\n",
    "        u_last_hidden_state = model.get_last_hidden_state(inputs_ids_a, segment_ids_a)\n",
    "        v_last_hidden_state = model.get_last_hidden_state(inputs_ids_b, segment_ids_b)\n",
    "\n",
    "        # Get the mean-pooled vectors - keep as 2D arrays for sklearn\n",
    "        u = mean_pool(u_last_hidden_state, attention_a).detach().cpu().numpy()  # (1, hidden_dim)\n",
    "        v = mean_pool(v_last_hidden_state, attention_b).detach().cpu().numpy()  # (1, hidden_dim)\n",
    "\n",
    "    # Calculate cosine similarity (sklearn expects 2D arrays)\n",
    "    similarity_score = cosine_similarity(u, v)[0, 0]\n",
    "\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check what embeddings look like with CUSTOM TOKENIZER\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "\n",
    "# Test with loaded_model and custom_tokenizer\n",
    "loaded_model.eval()\n",
    "inputs_a = custom_tokenizer(sentence_a, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
    "inputs_b = custom_tokenizer(sentence_b, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
    "\n",
    "inputs_a['input_ids'] = inputs_a['input_ids'].to(device)\n",
    "inputs_a['attention_mask'] = inputs_a['attention_mask'].to(device)\n",
    "inputs_b['input_ids'] = inputs_b['input_ids'].to(device)\n",
    "inputs_b['attention_mask'] = inputs_b['attention_mask'].to(device)\n",
    "\n",
    "segment_ids_a = torch.zeros_like(inputs_a['input_ids']).to(device)\n",
    "segment_ids_b = torch.zeros_like(inputs_b['input_ids']).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_hidden = loaded_model.get_last_hidden_state(inputs_a['input_ids'], segment_ids_a)\n",
    "    v_hidden = loaded_model.get_last_hidden_state(inputs_b['input_ids'], segment_ids_b)\n",
    "    \n",
    "    u = mean_pool(u_hidden, inputs_a['attention_mask']).detach().cpu().numpy()\n",
    "    v = mean_pool(v_hidden, inputs_b['attention_mask']).detach().cpu().numpy()\n",
    "\n",
    "print(f\"Embedding A shape: {u.shape}\")\n",
    "print(f\"Embedding B shape: {v.shape}\")\n",
    "print(f\"Embedding A first 10 values: {u[0, :10]}\")\n",
    "print(f\"Embedding B first 10 values: {v[0, :10]}\")\n",
    "print(f\"Are embeddings identical? {np.allclose(u, v)}\")\n",
    "print(f\"Max value in A: {u.max():.4f}, Min: {u.min():.4f}\")\n",
    "print(f\"Max value in B: {v.max():.4f}, Min: {v.min():.4f}\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(u, v)[0, 0]\n",
    "print(f\"\\nCosine Similarity: {similarity:.4f}\")\n",
    "print(\"\\nNow using CUSTOM TOKENIZER from Task 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd040dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Similar meaning between two sentences (using custom tokenizer)\n",
    "sentence_a = 'Authorities have announced a national holiday today.'\n",
    "sentence_b = \"Authorities have announced that today is a national holiday.\"\n",
    "similarity = calculate_similarity(loaded_model, custom_tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fe8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check what embeddings look like\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "\n",
    "# Test with loaded_model\n",
    "loaded_model.eval()\n",
    "inputs_a = custom_tokenizer(sentence_a, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "inputs_b = custom_tokenizer(sentence_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "\n",
    "segment_ids_a = torch.zeros_like(inputs_a['input_ids']).to(device)\n",
    "segment_ids_b = torch.zeros_like(inputs_b['input_ids']).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_hidden = loaded_model.get_last_hidden_state(inputs_a['input_ids'], segment_ids_a)\n",
    "    v_hidden = loaded_model.get_last_hidden_state(inputs_b['input_ids'], segment_ids_b)\n",
    "    \n",
    "    u = mean_pool(u_hidden, inputs_a['attention_mask']).detach().cpu().numpy()\n",
    "    v = mean_pool(v_hidden, inputs_b['attention_mask']).detach().cpu().numpy()\n",
    "\n",
    "print(f\"Embedding A shape: {u.shape}\")\n",
    "print(f\"Embedding B shape: {v.shape}\")\n",
    "print(f\"Embedding A first 10 values: {u[0, :10]}\")\n",
    "print(f\"Embedding B first 10 values: {v[0, :10]}\")\n",
    "print(f\"Are embeddings identical? {np.allclose(u, v)}\")\n",
    "print(f\"Max value in A: {u.max():.4f}, Min: {u.min():.4f}\")\n",
    "print(f\"Max value in B: {v.max():.4f}, Min: {v.min():.4f}\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(u, v)[0, 0]\n",
    "print(f\"\\nCosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498b954",
   "metadata": {},
   "source": [
    "## Task 3. Evaluation and Analysis (1 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1775ad",
   "metadata": {},
   "source": [
    "### 1) Provide the performance metrics (classification Report) based on the SNLI or MNLI datasets for the Natural Language Inference (NLI) task.\n",
    "\n",
    "|precision |recall |f1-score |support|\n",
    "|----------|-------|---------|-------|\n",
    "entailment| 0.42| 0.02 |0.05 |3486\n",
    "neutral |0.33 |0.75 |0.46 |3199\n",
    "contradiction |0.33 |0.25 |0.28 |3315\n",
    "accuracy | |0.33 |10000\n",
    "macro avg |0.36 |0.34 |0.26 |10000\n",
    "weighted avg| 0.36 |0.33 |0.26 |10000\n",
    "\n",
    "Table 1. Sample of Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "pre_trained_model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2185de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentence = [\"Authorities have announced a national holiday today.\", \"Authorities have announced that today is a national holiday.\"]\n",
    "opp_sentence = [\"Your contribution helped make it possible for us to provide our students with a quality education.\", \"Your contributions were of no help with our students' education.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc28a9",
   "metadata": {},
   "source": [
    "Positive Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(pos_sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = pre_trained_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7cc59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "sent_a_emb = sentence_embeddings[0].cpu().numpy().reshape(1, -1)\n",
    "sent_b_emb = sentence_embeddings[1].cpu().numpy().reshape(1, -1)\n",
    "cosine_similarity(sent_a_emb, sent_b_emb)[0][0]\n",
    "print(f\"Cosine Similarity Positive setences: {cosine_similarity(sent_a_emb, sent_b_emb)[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae39ce4",
   "metadata": {},
   "source": [
    "Opposite setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(opp_sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = pre_trained_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "sent_a_emb = sentence_embeddings[0].cpu().numpy().reshape(1, -1)\n",
    "sent_b_emb = sentence_embeddings[1].cpu().numpy().reshape(1, -1)\n",
    "cosine_similarity(sent_a_emb, sent_b_emb)[0][0]\n",
    "print(f\"Cosine Similarity Opposite setences: {cosine_similarity(sent_a_emb, sent_b_emb)[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12ca8c",
   "metadata": {},
   "source": [
    "### 2) Discuss any limitations or challenges encountered during the implementation and propose potential improvements or modifications.\n",
    "\n",
    "NOTE: Make sure to provide proper documentation, including details of the datasets used, hyperparameters,\n",
    "and any modifications made to the original models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fd740",
   "metadata": {},
   "source": [
    "### <font color=\"red\">ANSWER: </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2714a92",
   "metadata": {},
   "source": [
    "## Task 4. Text similarity - Web Application Development - Develop a simple web application that\n",
    "demonstrates the capabilities of your text-embedding model. (1 points)\n",
    "1) Develop a simple website with two input boxes for search queries.\n",
    "2) Utilize a custom-trained sentence transformer model to predict Natural Language Inference (NLI) Task (entailment, neutral and contradiction).\n",
    "\n",
    "For example:\n",
    "\n",
    "- Premise: A man is playing a guitar on stage.\n",
    "- Hypothesis: The man is performing music.\n",
    "- Label: Entailment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
