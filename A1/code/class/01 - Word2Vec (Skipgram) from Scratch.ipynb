{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word2Vec\n",
        "\n",
        "Let's work on skipgram-based implementation of word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for numerical computations\n",
        "import numpy as np\n",
        "# neural network libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# for plotting\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('2.4.1', '2.9.1')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.__version__, torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.10.8'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib\n",
        "matplotlib.__version__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Define some very simple data for understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# window size is 1 (one word to the left and one to the right)\n",
        "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\",\n",
        "                 \"dog cat animal\", \"cat animal dog\", \"cat dog animal\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['apple', 'banana', 'fruit'],\n",
              " ['banana', 'apple', 'fruit'],\n",
              " ['banana', 'fruit', 'apple'],\n",
              " ['dog', 'cat', 'animal'],\n",
              " ['cat', 'animal', 'dog'],\n",
              " ['cat', 'dog', 'animal']]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. tokenization - basic tokenization by splitting on spaces\n",
        "# tokenization is based on words here - there are advanced tokenization techniques like using root words, prefixes, suffixes, etc.\n",
        "corpus = [sent.split(\" \") for sent in corpus]\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['banana', 'apple', 'fruit', 'dog', 'cat', 'animal']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get word sequences \n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "# get unique words and \n",
        "vocab = list(set(flatten(corpus))) # vocabs = all unique words in the corpus\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'banana': 0, 'apple': 1, 'fruit': 2, 'dog': 3, 'cat': 4, 'animal': 5}\n"
          ]
        }
      ],
      "source": [
        "# numericalization\n",
        "# create handy mapping between integers and words\n",
        "# done by creaing disctionary\n",
        "word2index = {w: i for i, w in enumerate(vocab)}\n",
        "print(word2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "#vocab size\n",
        "voc_size = len(vocab)\n",
        "print(voc_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#append UNK\n",
        "vocab.append('<UNK>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['banana', 'apple', 'fruit', 'dog', 'cat', 'animal', '<UNK>']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2index['<UNK>'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '<UNK>', 1: 'apple', 2: 'fruit', 3: 'dog', 4: 'cat', 5: 'animal'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#just in case we need to use , index to word\n",
        "index2word = {v:k for k, v in word2index.items()} \n",
        "index2word"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['apple', 'banana', 'fruit']\n",
            "['banana', 'apple', 'fruit']\n",
            "['banana', 'fruit', 'apple']\n",
            "['dog', 'cat', 'animal']\n",
            "['cat', 'animal', 'dog']\n",
            "['cat', 'dog', 'animal']\n"
          ]
        }
      ],
      "source": [
        "for c in corpus:\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create pais of center word and context word (outside words)\n",
        "# loop each corpus\n",
        "    # loop each document\n",
        "        # look from the 2nd word unitl second last word\n",
        "            # get the center word\n",
        "            # outside words = 2 words (left and right of center word)\n",
        "            # for each of these two outside words, we gonna append to a list\n",
        "                # center, outside1; center, outside2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_batch(batch_size, word_sequence):\n",
        "    \n",
        "    # Make skip gram of one size window\n",
        "    skip_grams = []\n",
        "    # loop each word sequence\n",
        "    # we starts from 1 because 0 has no context or complete context (left and right)\n",
        "    # we stop at second last for the same reason - has no complete context\n",
        "    for sent in corpus:\n",
        "        for i in range(1, len(sent) - 1):\n",
        "            target = word2index[sent[i]] # center word in integers\n",
        "            context = [word2index[sent[i - 1]], word2index[sent[i + 1]]] # outside words in integers\n",
        "            for w in context:\n",
        "                skip_grams.append([target, w])\n",
        "    \n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
        "        \n",
        "    for i in random_index:\n",
        "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
        "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
        "            \n",
        "    return np.array(random_inputs), np.array(random_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  [[5]\n",
            " [2]]\n",
            "Target:  [[4]\n",
            " [0]]\n"
          ]
        }
      ],
      "source": [
        "#testing the method\n",
        "batch_size = 2 # mini-batch size\n",
        "input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "\n",
        "print(\"Input: \", input_batch)\n",
        "print(\"Target: \", target_batch)\n",
        "#we will convert them to tensor during training, so don't worry..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((2, 1), (2, 1))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch.shape, target_batch.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
        "\n",
        "where $P(w_{t+j} | w_t; \\theta) = $\n",
        "\n",
        "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
        "\n",
        "where $o$ is the outside words and $c$ is the center word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Skipgram(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(Skipgram,self).__init__()\n",
        "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
        "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
        "    \n",
        "    def forward(self, center_words, target_words, all_vocabs):\n",
        "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
        "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
        "        all_embeds    = self.embedding_u(all_vocabs) #   [batch_size, voc_size, emb_size]\n",
        "        \n",
        "        scores      = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
        "\n",
        "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #[batch_size, voc_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, voc_size, 1] = [batch_size, voc_size]\n",
        "\n",
        "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
        "        # scalar (loss must be scalar)    \n",
        "            \n",
        "        return nll # negative log likelihood\n",
        "\n",
        "# embedding shape => batch_size, em"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size     = 2 # mini-batch size\n",
        "embedding_size = 2 #so we can later plot\n",
        "model          = Skipgram(voc_size, embedding_size)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 7])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return torch.LongTensor(idxs)\n",
        "\n",
        "#use for the normalized term in the probability calculation\n",
        "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab))  # [batch_size, voc_size]\n",
        "all_vocabs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1000 | cost: 1.541784 | time: 0m 0s\n",
            "Epoch: 2000 | cost: 1.703758 | time: 0m 0s\n",
            "Epoch: 3000 | cost: 1.610825 | time: 0m 0s\n",
            "Epoch: 4000 | cost: 0.844709 | time: 0m 0s\n",
            "Epoch: 5000 | cost: 0.794343 | time: 0m 0s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Training\n",
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # get batch data\n",
        "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
        "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
        "\n",
        "    # predict loss\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(input_batch, target_batch, all_vocabs)\n",
        "    \n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "    \n",
        "    # update alpha\n",
        "    optimizer.step()\n",
        "    \n",
        "    end = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plotting the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is fruit near to banana? Is fruit far from cat ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['banana', 'apple', 'fruit', 'dog', 'cat', 'animal', '<UNK>']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#list of vocabs\n",
        "vocab[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "banana = torch.tensor([[word2index['banana']]])\n",
        "banana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-1.6616, -0.7033]]], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([[[ 1.4764, -1.9660]]], grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "banana_embed_v = model.embedding_v(banana)\n",
        "banana_embed_u = model.embedding_u(banana)\n",
        "banana_embed_v, banana_embed_u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "banana_embed =  (banana_embed_v + banana_embed_u) /2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'banana'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word = vocab[0]\n",
        "word "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#numericalization\n",
        "id = word2index[word]\n",
        "id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_tensor = torch.LongTensor([id])\n",
        "id_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-1.1529, -1.2574]], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([[-1.0543, -0.4413]], grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get the embedding by averaging\n",
        "v_embed = model.embedding_v(id_tensor)\n",
        "u_embed = model.embedding_u(id_tensor)\n",
        "\n",
        "v_embed, u_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.8493, grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#average to get the word embedding\n",
        "word_embed = (v_embed + u_embed) / 2\n",
        "word_embed[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embed_chakyvideo(word):\n",
        "    try:\n",
        "        index = word2index[word]\n",
        "    except:\n",
        "        index = word2index['<UNK>']\n",
        "\n",
        "    word = torch.LongTensor([word2index[word]])\n",
        "\n",
        "    embed_v = model.embedding_v(word)\n",
        "    embed_u = model.embedding_u(word)\n",
        "    embed = (embed_v + embed_u) / 2\n",
        "\n",
        "    return embed[0][0].item(), embed[0][1].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#let's write a function to get embedding given a word\n",
        "def get_embed(word):\n",
        "    id_tensor = torch.LongTensor([word2index[word]])\n",
        "    v_embed = model.embedding_v(id_tensor)\n",
        "    u_embed = model.embedding_u(id_tensor) \n",
        "    word_embed = (v_embed + u_embed) / 2 \n",
        "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.2314226627349854, 0.6353347301483154)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_embed('fruit')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.348540186882019, 0.4124754071235657)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_embed('banana')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-1.3295440673828125, -0.48725149035453796)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_embed('cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-1.103606939315796, -0.8493368029594421)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_embed('dog')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(6,3))\n",
        "for i, word in enumerate(vocab[:20]): #loop each unique vocab\n",
        "    x, y = get_embed(word)\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cosine similarity\n",
        "\n",
        "Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
        "\n",
        "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ \n",
        "\n",
        "If $p$ and $q$ is super similar, the result is 1 otherwise 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog', 'fruit', 'apple', 'cat', 'animal', 'banana', '<UNK>']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#let's try similarity between first and second, and second and third\n",
        "cat          = get_embed('cat')\n",
        "fruit        = get_embed('fruit')\n",
        "animal       = get_embed('animal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(-1.9467984898188107)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(cat) @ np.array(fruit) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.744612371516169)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(cat) @ np.array(animal) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat vs. fruit:  -0.9921933944854652\n",
            "cat vs. animal:  0.8857921869342228\n",
            "cat vs. cat:  1.0\n"
          ]
        }
      ],
      "source": [
        "#numpy version\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "    return cos_sim\n",
        "    \n",
        "print(f\"cat vs. fruit: \",        cos_sim(cat, fruit))\n",
        "print(f\"cat vs. animal: \",       cos_sim(cat, animal))\n",
        "print(f\"cat vs. cat: \",          cos_sim(cat, cat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat vs. fruit:  -0.9921933944854651\n",
            "cat vs. animal:  0.8857921869342228\n",
            "cat vs. cat:  1.0\n"
          ]
        }
      ],
      "source": [
        "#scipy version\n",
        "from scipy import spatial\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    cos_sim = 1 - spatial.distance.cosine(a, b)  #distance = 1 - similarlity, because scipy only gives distance\n",
        "    return cos_sim\n",
        "\n",
        "print(f\"cat vs. fruit: \",        cos_sim(cat, fruit))\n",
        "print(f\"cat vs. animal: \",       cos_sim(cat, animal))\n",
        "print(f\"cat vs. cat: \",          cos_sim(cat, cat))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "assignment-npl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
