{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f647871",
   "metadata": {},
   "source": [
    "# A3: Make Your Own Machine Translation \n",
    "\n",
    "In this assignment, we will explore the domain of neural machine translation. The focus will be on\n",
    "translating between your native language and English. We will experiment with different types of attention\n",
    "mechanisms, including general attention, multiplicative attention, and additive attention, to evaluate their\n",
    "effectiveness in the translation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0491c",
   "metadata": {},
   "source": [
    "#### Step 0: Prepare Environment - Import Libraries and select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ac4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import datasets, math, re, random, time\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1840dc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mimimum required torch version for MPS support \"1.12+\"\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c605dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# universal device selection: use gpu if available, else cpu\n",
    "import torch\n",
    "\n",
    "# def get_device():\n",
    "#     if torch.cuda.is_available():\n",
    "#         return torch.device(\"cuda\")      # NVIDIA GPU\n",
    "#     elif torch.backends.mps.is_available():\n",
    "#         return torch.device(\"mps\")       # Apple Silicon GPU\n",
    "#     else:\n",
    "#         return torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CPU preferred, as MPS keeps on crashing during training with memory errors.\n",
    "#RuntimeError: MPS backend out of memory (MPS allocated: 86.95 GiB, \n",
    "# other allocations: 1.14 GiB, max allowed: 88.13 GiB). Tried to allocate 42.25 MiB on private pool. \n",
    "# Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b656b",
   "metadata": {},
   "source": [
    "## Task 1. Get Language Pair - Based on MT + Transformer.ipynb, modify the dataset as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87b5b5",
   "metadata": {},
   "source": [
    "### 1.1) Find a dataset suitable for translation between your native language and English. Ensure to source this dataset from reputable public databases or repositories. It is imperative to give proper credit to the dataset source in your documentation. (1 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e49d4b",
   "metadata": {},
   "source": [
    "Context Behind Nepali Language:\n",
    "\n",
    "My native language is Nepali. It is spoken by roughly 32 million people around the world as first or second language. \n",
    "Percentage estimate is ~0.4% (total estimated world population 8 Billion people).\n",
    "\n",
    "There's few coproa exists on Nepali to English translation on Hugging Face.\n",
    "1. Opus project [Helsinki NLP Research](https://huggingface.co/datasets/Helsinki-NLP/opus-100/viewer/en-ne)\n",
    "\n",
    "Helsinki-NLP refers to the language technology research group at the University of Helsinki. Here, we publish various resource related to multilingual NLP, machine translation, text simplification to name a few application areas. We focus on wide language coverage, open data sets and public pre-trained models.\n",
    "\n",
    "2. [IRIIS project](https://huggingface.co/IRIIS-RESEARCH)\n",
    "IRIIS-Research is a research group that publishes large-scale raw text corpora on Hugging Face, including one of the largest publicly available Nepali text datasets. This is monolingual Nepali text and large dataset in GBs (~10 GB). \n",
    "\n",
    "3. [ERLA](https://catalog.elra.info/en-us/repository/search/?q=nepali)\n",
    "\n",
    "Founded in 1995, ELRA, the ELRA Language Resources Association is a non-profit organisation whose main mission is to make Language Resources (LRs) for Human Language Technologies (HLT) available to the community at large.\n",
    "\n",
    "4. [FLORES+](https://huggingface.co/datasets/openlanguagedata/flores_plus)\n",
    "FLORES+ is a multilingual machine translation benchmark released under CC BY-SA 4.0. This dataset was originally released by FAIR researchers at Meta under the name FLORES. Further information about these initial releases can be found in Dataset Sources below. The data is now being managed by OLDI, the Open Language Data Initiative. The + has been added to the name to disambiguate between the original datasets and this new actively developed version.\n",
    "\n",
    "Archived Flores : [Flores 200](https://huggingface.co/datasets/facebook/flores/blob/main/README.md#dataset-card-for-flores-200)\n",
    "\n",
    "More on research paper [Natural language processing for Nepali text: a review](../resources/Shahi-Sitaula2021_Article_NaturalLanguageProcessingForNe.pdf)\n",
    "\n",
    "<strong>For the assignment purpose, I am using smaller dataset from hugging face OPUS-100</strong>\n",
    "\n",
    "Opus chosen for:\n",
    "1. Better quality\n",
    "2. Proper train/val/test split\n",
    "3. Managable size with subset\n",
    "4. More realistic results\n",
    "\n",
    "** Tatoeba dataset is smaller good for fast training but it has simple sentences, and may not generalize well.\n",
    "\n",
    "\n",
    "| Dataset     | Size                | Languages      | Use Case                | Quality                  |\n",
    "|-------------|---------------------|----------------|-------------------------|--------------------------|\n",
    "| WMT14       | ~4.5M pairs (de-en) | 2-6 pairs      | Training                | High (news)              |\n",
    "| WMT16       | ~4.5M pairs         | 6-8 pairs      | Training                | High (news)              |\n",
    "| WMT19       | ~38M pairs (de-en)  | 10+ pairs      | Training                | High (news)              |\n",
    "| OPUS-100    | ~55M total          | 100 languages  | Multilingual training   | Medium                   |\n",
    "| Tatoeba     | ~10M total          | 400+ languages | Evaluation/Small training| Medium (user-contributed)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911ceb7",
   "metadata": {},
   "source": [
    "#### Step 1: Data preparation - using OPUS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e0c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Language Pair: en-ne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "EN_LANGUAGE = 'en'\n",
    "NE_LANGUAGE = 'ne'\n",
    "# Use \"de-en\" as dataset doesn't have en-de and treat English as source, German as target.\n",
    "LANG_PAIR = f\"{EN_LANGUAGE}-{NE_LANGUAGE}\"\n",
    "print(\"Translation Language Pair:\", LANG_PAIR)\n",
    "\n",
    "dataset = load_dataset(\"opus100\", LANG_PAIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb26ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 406381\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94317c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'The following item is due:',\n",
       "  'ne': 'निम्न वस्तुको म्याद समाप्त हुन्छ:'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde4dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 100000, Val: 2000, Test: 2000\n"
     ]
    }
   ],
   "source": [
    "# Selecting smaller subsets for faster training/testing\n",
    "train = dataset[\"train\"].select(range(100_000))  # 100K samples\n",
    "val = dataset[\"validation\"]\n",
    "test = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b585582f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'translation': {'en': '_Inv', 'ne': 'Inv'}},\n",
       " {'translation': {'en': '%1: the message is displayed silently.',\n",
       "   'ne': '% 1: सन्देश ध्वनि बिना प्रदर्शित हुन्छ ।'}},\n",
       " {'translation': {'en': 'Delete Thread', 'ne': 'थ्रेड मेट्नुहोस्'}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0], val[0], test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106dfac7",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2) Describe in detail the process of preparing the dataset for use in your translation model. This\n",
    "includes steps like text normalization, tokenization, and word segmentation, particularly focusing\n",
    "on your native language’s specific requirements. Specify the libraries or tools you will use for these\n",
    "tasks and give appropriate credit to the developers or organizations behind these tools. If your\n",
    "native language requires special handling in tokenization (e.g., for languages like Chinese, Thai, or\n",
    "Japanese), mention the libraries (like Jieba, PyThaiNLP, or Mecab) and the procedures used for\n",
    "word segmentation. (1 points)\n",
    "Note: proper attribution for both the dataset and the tools used in its processing is essential for maintaining\n",
    "academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d82727",
   "metadata": {},
   "source": [
    "#### Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fcda0",
   "metadata": {},
   "source": [
    "\n",
    "For English tokenization , pretarined model is used:\n",
    "\n",
    "```bash\n",
    "uv add spacy\n",
    "uv add pip\n",
    "\n",
    "uv run python3 -m spacy download en_core_web_sm \n",
    "```\n",
    "\n",
    "Instead of downloading using uv python, using python script download and save to local.\n",
    "\n",
    "There's no spaCy model for Nepali. For tokenization, \n",
    "\n",
    "1. use tokenizing algorithm and train on data \n",
    "    \n",
    "    Pros: Customized according to data, handles OOV\n",
    "    \n",
    "    Cons: Need to train\n",
    "\n",
    "\n",
    "SentencePiece is a tokenization algorithm (BPE or unigram) - it's simpler and faster.\n",
    "\n",
    "2. Use pretrained tokenizer\n",
    "    \n",
    "    Pros: Ready to use\n",
    "\n",
    "    Cons: May not fit to dataset used\n",
    "\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Already trained, supports Nepali!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "```\n",
    "\n",
    "Using pre-trained model from Meta [NLLB](https://github.com/facebookresearch/fairseq/tree/nllb)\n",
    "\n",
    "Research Paper : [No Language Left Behind:Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672)\n",
    "\n",
    "\n",
    "\n",
    "Create two dictionaries 1. for holding our tokenizers and 2. for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc56da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d1045e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_DIRECTORY = \"./../models\"\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(MODEL_DIRECTORY, exist_ok=True)\n",
    "\n",
    "SPACY_MODEL_PATH = os.path.join(MODEL_DIRECTORY, \"en_core_web_sm\")\n",
    "\n",
    "def load_spacy_model():\n",
    "    \"\"\"Load spaCy model from custom directory, download if needed\"\"\"\n",
    "    # Check if config.cfg exists (proper model structure)\n",
    "    if os.path.exists(os.path.join(SPACY_MODEL_PATH, \"config.cfg\")):\n",
    "        return spacy.load(SPACY_MODEL_PATH)\n",
    "    \n",
    "    # Download and copy to custom folder\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    download(\"en_core_web_sm\")\n",
    "    \n",
    "    import en_core_web_sm\n",
    "    source_path = en_core_web_sm.__path__[0]\n",
    "    \n",
    "    # Find the actual model directory (contains config.cfg)\n",
    "    # It's usually nested like: en_core_web_sm/en_core_web_sm-3.x.x/\n",
    "    # Config checks added to fix - OSError: [E053] Could not read config file from ../models/en_core_web_sm/config.cfg\n",
    "    config_files = glob.glob(os.path.join(source_path, \"**\", \"config.cfg\"), recursive=True)\n",
    "    if config_files:\n",
    "        actual_model_dir = os.path.dirname(config_files[0])\n",
    "    else:\n",
    "        actual_model_dir = source_path\n",
    "    \n",
    "    # Copy the actual model files\n",
    "    os.makedirs(MODEL_DIRECTORY, exist_ok=True)\n",
    "    if os.path.exists(SPACY_MODEL_PATH):\n",
    "        shutil.rmtree(SPACY_MODEL_PATH)\n",
    "    shutil.copytree(actual_model_dir, SPACY_MODEL_PATH)\n",
    "    \n",
    "    # Load spaCy models directly (faster than get_tokenizer for batch processing)\n",
    "    return spacy.load(SPACY_MODEL_PATH, disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "\n",
    "def load_nllb():\n",
    "    \"\"\"Load NLLB tokenizer from custom directory, download if needed\"\"\"\n",
    "    nllb_path = os.path.join(MODEL_DIRECTORY, \"nllb-tokenizer\")\n",
    "    if os.path.exists(nllb_path):\n",
    "        return AutoTokenizer.from_pretrained(nllb_path)\n",
    "    \n",
    "    print(\"Downloading NLLB tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "    tokenizer.save_pretrained(nllb_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec51eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = load_spacy_model()\n",
    "\n",
    "# Add English tokenizer to token_transform\n",
    "def spacy_tokenize(text):\n",
    "    \"\"\"Tokenize text using spaCy\"\"\"\n",
    "    return [tok.text for tok in spacy_model(text)]\n",
    "\n",
    "token_transform[EN_LANGUAGE] = spacy_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "321e56c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁नेपाल', '▁सुन्दर', '▁छ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "nllb = load_nllb()\n",
    "sample_nepali_sentence = \"नेपाल सुन्दर छ\"\n",
    "\n",
    "# Add Nepali tokenizer to token_transform\n",
    "token_transform[NE_LANGUAGE] = nllb.tokenize\n",
    "\n",
    "print(token_transform[NE_LANGUAGE](sample_nepali_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cce3e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: निम्न वस्तुको म्याद समाप्त हुन्छ:\n",
      "Tokenization:  ['▁निम्न', '▁वस्तु', 'को', '▁म्या', 'द', '▁समाप्त', '▁हुन्छ', ':']\n"
     ]
    }
   ],
   "source": [
    "train_nepali_text = train[100]['translation'][NE_LANGUAGE]\n",
    "print(\"Sentence:\", train_nepali_text)\n",
    "print(\"Tokenization: \", token_transform[NE_LANGUAGE](train_nepali_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "334ee1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The following item is due:\n",
      "Tokenization:  ['The', 'following', 'item', 'is', 'due', ':']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "train_english_text = train[100]['translation'][EN_LANGUAGE]\n",
    "print(\"Sentence:\", train_english_text)\n",
    "print(\"Tokenization: \", token_transform[EN_LANGUAGE](train_english_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78d0cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = EN_LANGUAGE\n",
    "TRG_LANGUAGE = NE_LANGUAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527850d",
   "metadata": {},
   "source": [
    "Function to token input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b1a7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample['translation'][language])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121201d0",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence.\n",
    "\n",
    "special symbols `<unk>`, `<pad>`, `<sos>`, `<eos>` with indexes 0, 1, 2, 3 respectively. Where each symbol has meanings as such:\n",
    ">\n",
    ">   `<unk>`: To represent Unknown\n",
    ">\n",
    ">   `<pad>`: Padding, used to ensure all sequences are of same length\n",
    ">\n",
    ">   `<sos>`: Start of sentence\n",
    ">\n",
    ">   `<eos>`: End of sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e53ef6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5cf598",
   "metadata": {},
   "source": [
    "#### Step 4: Numericalization\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we build Vocab class as torchtext.vocab is not supported in Python v3.13+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3950602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.vocab replacement - Vocab class to mimic torchtext API\n",
    "from collections import Counter\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"A simple Vocab class to replace torchtext.vocab\"\"\"\n",
    "    def __init__(self, stoi, itos, default_index=0):\n",
    "        self.stoi = stoi  # string to index\n",
    "        self.itos = itos  # index to string (list)\n",
    "        self.default_index = default_index\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        \"\"\"Convert list of tokens to list of indices\"\"\"\n",
    "        return [self.stoi.get(token, self.default_index) for token in tokens]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.default_index)\n",
    "    \n",
    "    def set_default_index(self, index):\n",
    "        self.default_index = index\n",
    "    \n",
    "    def get_itos(self):\n",
    "        return self.itos\n",
    "\n",
    "def build_vocab(token_iterator, min_freq=2, specials=None, special_first=True):\n",
    "    \"\"\"Build vocabulary from token iterator\"\"\"\n",
    "    if specials is None:\n",
    "        specials = []\n",
    "    \n",
    "    # Count token frequencies\n",
    "    counter = Counter()\n",
    "    for tokens in token_iterator:\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # Build itos (index to string) list\n",
    "    itos = []\n",
    "    if special_first:\n",
    "        itos.extend(specials)\n",
    "    \n",
    "    # Add tokens that meet min_freq threshold\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq and token not in specials:\n",
    "            itos.append(token)\n",
    "    \n",
    "    if not special_first:\n",
    "        itos.extend(specials)\n",
    "    \n",
    "    # Build stoi (string to index) dict\n",
    "    stoi = {token: idx for idx, token in enumerate(itos)}\n",
    "    \n",
    "    return Vocab(stoi, itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3897997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab for en (batch mode)...\n",
      "Building vocab for ne...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting EN texts: 100%|██████████| 100000/100000 [00:00<00:00, 174785.75it/s]\n",
      "Tokenizing EN:   2%|▏         | 2001/100000 [00:02<01:25, 1144.59it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3645 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing NE: 100%|██████████| 100000/100000 [00:03<00:00, 30908.52it/s]\n",
      "Tokenizing EN:   3%|▎         | 3001/100000 [00:02<01:17, 1247.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ne done in 3.3s, vocab size: 8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing EN: 100%|██████████| 100000/100000 [01:02<00:00, 1593.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  en done in 63.3s, vocab size: 15836\n",
      "\n",
      "Total time: 63.3s\n",
      "en vocab size: 15836\n",
      "ne vocab size: 8333\n",
      "NLLB tokenizer warning about max sequence length may be ignored for vocab building. \n",
      "It is only used as a tokenizer here. The 1024 limit applies during model training/inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_vocab_en_fast(data):\n",
    "    \"\"\"Build English vocab using spaCy's fast pipe() method\"\"\"\n",
    "    print(f\"Building vocab for {EN_LANGUAGE} (batch mode)...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Collect all English texts\n",
    "    texts = [sample['translation'][EN_LANGUAGE] for sample in tqdm(data, desc=\"Collecting EN texts\")]\n",
    "    \n",
    "    # Batch tokenize with spaCy pipe() - MUCH faster!\n",
    "    counter = Counter()\n",
    "    for doc in tqdm(spacy_model.pipe(texts, batch_size=1000, n_process=1), \n",
    "                    total=len(texts), desc=\"Tokenizing EN\"):\n",
    "        counter.update([tok.text for tok in doc])\n",
    "    \n",
    "    # Build vocab\n",
    "    itos = list(special_symbols)\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= 2 and token not in special_symbols:\n",
    "            itos.append(token)\n",
    "    stoi = {token: idx for idx, token in enumerate(itos)}\n",
    "    \n",
    "    vocab = Vocab(stoi, itos)\n",
    "    vocab.set_default_index(UNK_IDX)\n",
    "    print(f\"  {EN_LANGUAGE} done in {time.time() - start:.1f}s, vocab size: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "def build_vocab_ne(data):\n",
    "    \"\"\"Build Nepali vocab (NLLB tokenizer)\"\"\"\n",
    "    print(f\"Building vocab for {NE_LANGUAGE}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    counter = Counter()\n",
    "    for sample in tqdm(data, desc=\"Tokenizing NE\"):\n",
    "        tokens = token_transform[NE_LANGUAGE](sample['translation'][NE_LANGUAGE])\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # Build vocab\n",
    "    itos = list(special_symbols)\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= 2 and token not in special_symbols:\n",
    "            itos.append(token)\n",
    "    stoi = {token: idx for idx, token in enumerate(itos)}\n",
    "    \n",
    "    vocab = Vocab(stoi, itos)\n",
    "    vocab.set_default_index(UNK_IDX)\n",
    "    print(f\"  {NE_LANGUAGE} done in {time.time() - start:.1f}s, vocab size: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "# Build vocabularies in parallel\n",
    "start_total = time.time()\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    future_en = executor.submit(build_vocab_en_fast, train)\n",
    "    future_ne = executor.submit(build_vocab_ne, train)\n",
    "    \n",
    "    vocab_transform[EN_LANGUAGE] = future_en.result()\n",
    "    vocab_transform[NE_LANGUAGE] = future_ne.result()\n",
    "\n",
    "print(f\"\\nTotal time: {time.time() - start_total:.1f}s\")\n",
    "print(f\"{SRC_LANGUAGE} vocab size: {len(vocab_transform[SRC_LANGUAGE])}\")\n",
    "print(f\"{TRG_LANGUAGE} vocab size: {len(vocab_transform[TRG_LANGUAGE])}\")\n",
    "\n",
    "# Warning : Token indices sequence length is longer than the specified maximum sequence length for this model (3645 > 1024). Running this sequence through the model will result in indexing errors\n",
    "print(\"\"\"NLLB tokenizer warning about max sequence length may be ignored for vocab building. \n",
    "It is only used as a tokenizer here. The 1024 limit applies during model training/inference.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3b18a",
   "metadata": {},
   "source": [
    "The parallalization addition didn't benefit much as the volume of Nepali dataset was not huge and NLLB is fast since it's implemented in Rust and it only took few seconds to get competed. \n",
    "\n",
    "The main time consuming tokenization was for EN language. After using `pipe()` for batch processing , the number reduced by more than 50%. It takes little more than 1 minute compared to ~3 minutes without batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fece20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2822, 289, 211, 0, 211]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4072856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 2822, for example\n",
    "mapping[2822]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311e3a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4694794c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1554346b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15836"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82847c1a",
   "metadata": {},
   "source": [
    "#### Step 5: Prepare data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ccb17",
   "metadata": {},
   "source": [
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "265d2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for sample in batch:\n",
    "        # OPUS-100 format: {'translation': {'en': '...', 'ne': '...'}}\n",
    "        src_sample = sample['translation'][SRC_LANGUAGE]\n",
    "        trg_sample = sample['translation'][TRG_LANGUAGE]\n",
    "        \n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088c811",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4095d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size for MPS memory constraints (64 -> 32)\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a99656",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec564dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, ne in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba6f7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([15, 32])\n",
      "Nepali shape:  torch.Size([22, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (seq len, batch_size)\n",
    "print(\"Nepali shape: \", ne.shape)   # (seq len, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10392f3a",
   "metadata": {},
   "source": [
    "## Task 2. Experiment with Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991eb68f",
   "metadata": {},
   "source": [
    "<strong>Implement a sequence-to-sequence neural network for the translation task.</strong> \n",
    "\n",
    "Note: For an in-depth exploration of attention mechanisms, you can refer to this $paper^1$.\n",
    "\n",
    "$^1$ An Attentive Survey of Attention Models https://arxiv.org/pdf/1904.02874.pdf\n",
    "\n",
    "Your implementation should include the following attention mechanisms, with their respective equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3c919",
   "metadata": {},
   "source": [
    "##### Step 6: Design Model :: Seq-to-Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "958c82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device  = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        #src: [src len, batch_size]\n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)  #permute so that it's the same shape as attention\n",
    "        #mask: [batch_size, src len] #(0, 0, 0, 0, 0, 1, 1)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src: [src len, batch_size]\n",
    "        #trg: [trg len, batch_size]\n",
    "        \n",
    "        #initialize something\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len    = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs    = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #send our src text into encoder\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        #encoder_outputs refer to all hidden states (last layer)\n",
    "        #hidden refer to the last hidden state (of each layer, of each direction)\n",
    "        \n",
    "        input_ = trg[0, :]\n",
    "        \n",
    "        mask   = self.create_mask(src) #(0, 0, 0, 0, 0, 1, 1)\n",
    "        \n",
    "        #for each of the input of the trg text\n",
    "        for t in range(1, trg_len):\n",
    "            #send them to the decoder\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output: [batch_size, output_dim] ==> predictions\n",
    "            #hidden: [batch_size, hid_dim]\n",
    "            #attention: [batch_size, src len]\n",
    "            \n",
    "            #append the output to a list\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1          = output.argmax(1)  #autoregressive\n",
    "            \n",
    "            input_ = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1870050",
   "metadata": {},
   "source": [
    "#### Step 7: Design Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bdf5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn       = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc        = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        #embedding\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #packed\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=False)\n",
    "        #rnn\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        #unpacked\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        #-1, -2 hidden state\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim = 1)))\n",
    "        \n",
    "        #outputs: [src len, batch_size, hid dim * 2]\n",
    "        #hidden:  [batch_size, hid_dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed28218",
   "metadata": {},
   "source": [
    "### 2.1) General Attention: (0.5 points)\n",
    "\n",
    "$$e_i = s^T h_i \\in \\mathbb{R} \\quad \\text{where} \\quad d_1 = d_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea30eb",
   "metadata": {},
   "source": [
    "#### Step 8: Design Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f68a96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    General (Dot-Product) Attention: e_i = s^T h_i\n",
    "    Requires projection to match dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        # Project encoder outputs to match decoder hidden dim\n",
    "        # enc_hid_dim is already the full encoder output dim (hid_dim * 2 for bidirectional)\n",
    "        self.proj = nn.Linear(enc_hid_dim, dec_hid_dim)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, enc_hid_dim]\n",
    "        \n",
    "        # Project encoder outputs to decoder dimension\n",
    "        encoder_projected = self.proj(encoder_outputs)  # [src_len, batch_size, dec_hid_dim]\n",
    "        encoder_projected = encoder_projected.permute(1, 0, 2)  # [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(2)  # [batch_size, dec_hid_dim, 1]\n",
    "        \n",
    "        # Dot product attention: s^T h\n",
    "        attention = torch.bmm(encoder_projected, hidden).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112803a",
   "metadata": {},
   "source": [
    "### 2.2) Multiplicative Attention: (0.5 points)\n",
    "\n",
    "$$e_i = s^T W h_i \\in \\mathbb{R} \\quad \\text{where} \\quad W \\in \\mathbb{R}^{d_2 \\times d_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13dd2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiplicative Attention: e_i = s^T W h_i\n",
    "    W is a learnable weight matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        # W matrix: maps encoder hidden to decoder hidden space\n",
    "        # enc_hid_dim is already the full encoder output dim (hid_dim * 2 for bidirectional)\n",
    "        self.W = nn.Linear(enc_hid_dim, dec_hid_dim, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, enc_hid_dim]\n",
    "        \n",
    "        # Apply W to encoder outputs\n",
    "        encoder_transformed = self.W(encoder_outputs)  # [src_len, batch_size, dec_hid_dim]\n",
    "        encoder_transformed = encoder_transformed.permute(1, 0, 2)  # [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(2)  # [batch_size, dec_hid_dim, 1]\n",
    "        \n",
    "        # s^T W h\n",
    "        attention = torch.bmm(encoder_transformed, hidden).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1f150",
   "metadata": {},
   "source": [
    "### 2.3) Additive Attention: (0.5 points)\n",
    "\n",
    "$$e_i = v^T \\tanh(W_1 h_i + W_2 s) \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1c78c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive (Bahdanau) Attention: e_i = v^T tanh(W1 h_i + W2 s)\n",
    "    Most flexible - different dimensions allowed\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        # enc_hid_dim is already the full encoder output dim (hid_dim * 2 for bidirectional)\n",
    "        self.W1 = nn.Linear(enc_hid_dim, dec_hid_dim)  # for encoder hidden\n",
    "        self.W2 = nn.Linear(dec_hid_dim, dec_hid_dim)  # for decoder hidden\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False) # to get scalar score\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, enc_hid_dim]\n",
    "        \n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        # Repeat hidden for each source position\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, enc_hid_dim]\n",
    "        \n",
    "        # v^T tanh(W1 h + W2 s)\n",
    "        energy = torch.tanh(self.W1(encoder_outputs) + self.W2(hidden))  # [batch_size, src_len, dec_hid_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5f42c",
   "metadata": {},
   "source": [
    "### Design Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a0c5b",
   "metadata": {},
   "source": [
    "#### Step 9: Design decoder to pass attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5db091f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention  = attention\n",
    "        self.embedding  = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn        = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc         = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        #input: [batch_size]\n",
    "        #hidden: [batch_size, hid_dim]\n",
    "        #encoder_ouputs: [src len, batch_size, hid_dim * 2]\n",
    "        #mask: [batch_size, src len]\n",
    "                \n",
    "        #embed our input\n",
    "        input    = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        #calculate the attention\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch_size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch_size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_ouputs: [batch_size, src len, hid_dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted: [batch_size, 1, hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted: [1, batch_size, hid_dim * 2]\n",
    "        \n",
    "        #send the input to decoder rnn\n",
    "            #concatenate (embed, weighted encoder_outputs)\n",
    "            #[1, batch_size, emb_dim]; [1, batch_size, hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input: [1, batch_size, emb_dim + hid_dim * 2]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "            \n",
    "        #send the output of the decoder rnn to fc layer to predict the word\n",
    "            #prediction = fc(concatenate (output, weighted, embed))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output   = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc(torch.cat((embedded, output, weighted), dim = 1))\n",
    "        #prediction: [batch_size, output_dim]\n",
    "            \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd53ac",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a83ba0",
   "metadata": {},
   "source": [
    "#### Step 10: Model Training\n",
    "\n",
    "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f8f0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0513261",
   "metadata": {},
   "source": [
    "Desgin mode to accept custom attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90a6e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with general attention...\n",
      "Seq2SeqPackedAttention(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(15836, 256)\n",
      "    (rnn): GRU(256, 512, bidirectional=True)\n",
      "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): GeneralAttention(\n",
      "      (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(8333, 256)\n",
      "    (rnn): GRU(1280, 512)\n",
      "    (fc): Linear(in_features=1792, out_features=8333, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Creating model with multiplicative attention...\n",
      "Seq2SeqPackedAttention(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(15836, 256)\n",
      "    (rnn): GRU(256, 512, bidirectional=True)\n",
      "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): MultiplicativeAttention(\n",
      "      (W): Linear(in_features=1024, out_features=512, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8333, 256)\n",
      "    (rnn): GRU(1280, 512)\n",
      "    (fc): Linear(in_features=1792, out_features=8333, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Creating model with additive attention...\n",
      "Seq2SeqPackedAttention(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(15836, 256)\n",
      "    (rnn): GRU(256, 512, bidirectional=True)\n",
      "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): AdditiveAttention(\n",
      "      (W1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (W2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8333, 256)\n",
      "    (rnn): GRU(1280, 512)\n",
      "    (fc): Linear(in_features=1792, out_features=8333, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attention_models = {}\n",
    "\n",
    "attention_models['general']        = GeneralAttention(hid_dim * 2, hid_dim)\n",
    "attention_models['multiplicative'] = MultiplicativeAttention(hid_dim * 2, hid_dim)\n",
    "attention_models['additive']       = AdditiveAttention(hid_dim * 2, hid_dim)\n",
    "\n",
    "\n",
    "def get_model_with_attention(attention_type='general'):\n",
    "    attn = attention_models.get(attention_type)\n",
    "    \n",
    "    if attn is None:\n",
    "        raise ValueError(f\"Unknown attention type: {attention_type}\")\n",
    "    \n",
    "    enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "    dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "    model = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example: Create model with General Attention\n",
    "for attention_type in attention_models.keys():\n",
    "    print(f\"Creating model with {attention_type} attention...\")\n",
    "    model = get_model_with_attention(attention_type)\n",
    "    print(model)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0869af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4054016\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "   512\n",
      "2133248\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "14932736\n",
      "  8333\n",
      "______\n",
      "27562125\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb15718",
   "metadata": {},
   "source": [
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f11a2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb9e8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, src_length, trg) in enumerate(loader):\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Clear MPS cache periodically to prevent memory buildup\n",
    "        if device.type == 'mps' and i % 50 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7086504",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2bda5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, attentions = model(src, src_length, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb46a1",
   "metadata": {},
   "source": [
    "Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccd123c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b3447",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80bf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Clear MPS cache before training\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 3\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'{MODEL_DIRECTORY}/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    tqdm.write(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    tqdm.write(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    tqdm.write(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    # Clear cache after each epoch\n",
    "    if device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b5e40",
   "metadata": {},
   "source": [
    "## Task 3. Evaluation and Verification - For the final evaluation and verification, perform the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa3265",
   "metadata": {},
   "source": [
    "### 3.1) Compare the performance of these attention mechanisms in terms of translation accuracy, computational efficiency, and other relevant metrics. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba0b0e",
   "metadata": {},
   "source": [
    "### 3.2) Provide performance plots showing training and validation loss for each type of attention mechanism (General, Multiplicative, and Additive). These plots will help in visualizing and comparing the learning curves of different attention models. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5813e",
   "metadata": {},
   "source": [
    "### 3.3) Display the attention maps generated by your model. Attention maps are crucial for understanding how the model focuses on different parts of the input sequence while generating the translation. This visualization will offer insights into the interpretability of your model. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9b6ac",
   "metadata": {},
   "source": [
    "### 3.4) Analyze the results and discuss the effectiveness of the selected attention mechanism in translating between your native language and English. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f450a2",
   "metadata": {},
   "source": [
    "Note: Provide the performance table and graph to Readme.md GitHub as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f01fe3",
   "metadata": {},
   "source": [
    "## Task 4. Machine Translation - Web Application Development - Develop a simple web application that showcases the capabilities of your language model in machine translation. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd8b05",
   "metadata": {},
   "source": [
    "1) The application should feature an input box where users can enter a sentence or phrase in a source\n",
    "language.\n",
    "2) Based on the input, the model should generate and display the translated version in a target language.\n",
    "For example, if the input is ”Hello, how are you?” in English, the model might generate\n",
    "”Hola, ¿c´omo est´as?” in Spanish.\n",
    "3) Provide documentation on how the web application interfaces with the language model for machine\n",
    "translation.\n",
    "Note : Choose the most effective attention mechanism based on your experiments in Task 2.\n",
    "As always, the example Dash Project in the GitHub repository contains an example that you can follow\n",
    "(if you use the Dash framework)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf9d84",
   "metadata": {},
   "source": [
    "### Save Model and Vocabularies for Flask App\n",
    "\n",
    "Note: For enhancing UI, Vibe coding was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903890a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabularies for Flask app\n",
    "import os\n",
    "\n",
    "save_dir = \"models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "vocab_data = {\n",
    "    'en_stoi': vocab_transform[EN_LANGUAGE].stoi,\n",
    "    'en_itos': vocab_transform[EN_LANGUAGE].itos,\n",
    "    'ne_stoi': vocab_transform[NE_LANGUAGE].stoi,\n",
    "    'ne_itos': vocab_transform[NE_LANGUAGE].itos,\n",
    "}\n",
    "\n",
    "torch.save(vocab_data, os.path.join(save_dir, \"vocabs.pt\"))\n",
    "print(f\"Vocabularies saved to {save_dir}/vocabs.pt\")\n",
    "print(f\"  EN vocab size: {len(vocab_data['en_itos'])}\")\n",
    "print(f\"  NE vocab size: {len(vocab_data['ne_itos'])}\")\n",
    "print(f\"\\nModel saved to: {save_path}\")\n",
    "print(\"\\nTo run the Flask app:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
