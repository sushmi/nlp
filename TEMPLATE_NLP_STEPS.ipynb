{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99804a38",
   "metadata": {},
   "source": [
    "#### Step 0: Prepare Environment - Import Libraries and select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcf54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import datasets, math, re, random, time\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimimum required torch version for MPS support \"1.12+\"\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# universal device selection: use gpu if available, else cpu\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear CUDA cache to free up memory \n",
    "        return torch.device(\"cuda\")      # NVIDIA GPU\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()  # Clear MPS cache to avoid memory issues\n",
    "        return torch.device(\"mps\")       # Apple Silicon GPU\n",
    "    else:\n",
    "        torch.empty_cache()  # Clear CPU cache to free up memory\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21675d06",
   "metadata": {},
   "source": [
    "#### Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub login token\n",
    "# Make sure to set the HF_TOKEN environment variable in your .env file with your Hugging Face token\n",
    "import os\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd540135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub login token\n",
    "# Make sure to set the HF_TOKEN environment variable in your .env file with your Hugging Face token\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8400a85",
   "metadata": {},
   "source": [
    "Download corpus for first time , save it to load it from local next time\n",
    "\n",
    "This is example corpus downloaded from HUGGINGFACE. Load dataset as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf402fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# data folder are not uploaded to Github.\n",
    "_DATA_PATH = \"../data/wikitext-103\"\n",
    "_DATA_FILENAME = os.path.join(_DATA_PATH, \"wikitext-103-train.arrow\")\n",
    "os.makedirs(_DATA_PATH, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(_DATA_FILENAME):\n",
    "    # Download and save to local folder\n",
    "    dataset_train = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\", cache_dir=_DATA_PATH)\n",
    "    dataset_valid = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\", cache_dir=_DATA_PATH)\n",
    "    dataset_test = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\", cache_dir=_DATA_PATH)\n",
    "\n",
    "else:\n",
    "    # Load from local Parquet file\n",
    "    from datasets import Dataset\n",
    "    dataset_train = Dataset.from_parquet(_DATA_FILENAME)\n",
    "    dataset_valid = Dataset.from_parquet(os.path.join(_DATA_PATH, \"wikitext-103-validation.arrow\"))\n",
    "    dataset_test = Dataset.from_parquet(os.path.join(_DATA_PATH, \"wikitext-103-test.arrow\"))\n",
    "    print(\"Loaded datasets from local Parquet files.\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset_train)}\")\n",
    "print(f\"Validation set size: {len(dataset_valid)}\")\n",
    "print(f\"Test set size: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 entries in the dataset \n",
    "# with only the first 80 characters of the text for brevity\n",
    "[text[:80] for text in dataset_train[:5]['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070384e",
   "metadata": {},
   "source": [
    "#### Step 2: Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case, and clean all the symbols\n",
    "texts = [re.sub(\"[.,!?\\\\-]\", '', t.lower()) for t in dataset_train[\"text\"] if t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ad2a6",
   "metadata": {},
   "source": [
    "#### Step 3. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083707f",
   "metadata": {},
   "source": [
    "Depending on datasize, use either spacy or nltk\n",
    "\n",
    "spaCy is very slow. Better to default to NLTK for educational purposes. If dataset is smaller like below 100_000, use spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdef956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "sentences = []\n",
    "# batch processing with nlp.pipe for efficiency or performance\n",
    "for doc in nlp.pipe(texts, batch_size=1000):\n",
    "    sentences.append([sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35026353",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "_DOWNLOAD_DIR = \"../models/nltk_data\"\n",
    "os.makedirs(_DOWNLOAD_DIR, exist_ok=True)\n",
    "nltk.download('punkt', download_dir=_DOWNLOAD_DIR)\n",
    "nltk.download('punkt_tab', download_dir=_DOWNLOAD_DIR)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "texts = [re.sub(\"[.,!?\\\\-]\", '', t.lower()) for t in dataset_train[\"text\"] if t.strip()]\n",
    "\n",
    "nltk.data.path.append(_DOWNLOAD_DIR)\n",
    "tokenized_texts = [word_tokenize(text) for text in texts]\n",
    "\n",
    "# Example: print the first 5 tokenized samples\n",
    "for tokens in tokenized_texts[:5]:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b03b20",
   "metadata": {},
   "source": [
    "#### Step 3: Numericalization\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we build Vocab class as torchtext.vocab is not supported in Python v3.13+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a55fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50130744",
   "metadata": {},
   "source": [
    "#### Step 5: Prepare data loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57924a2",
   "metadata": {},
   "source": [
    "##### Step 6: Design Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836da25f",
   "metadata": {},
   "source": [
    "#### Step 7: Design Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb3899",
   "metadata": {},
   "source": [
    "#### Step 8: Design Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41c023",
   "metadata": {},
   "source": [
    "#### Step 9: Design decoder to pass attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e66f32",
   "metadata": {},
   "source": [
    "#### Step 10: Model Training\n",
    "\n",
    "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f338b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd51112",
   "metadata": {},
   "source": [
    "#### Step 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36cde8",
   "metadata": {},
   "source": [
    "#### Step 12. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e7044",
   "metadata": {},
   "source": [
    "#### Step 13: Save all models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
